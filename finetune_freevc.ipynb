{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning FreeVC\n",
    "\n",
    "\n",
    "Hi!\n",
    "\n",
    "This notebook takes you through the steps to **fine-tune** the **Voice Conversion** model **FreeVC**. It is meant to be beginner-friendly, sparing you (and myself, the author of this notebook) most of the details of FreeVC's incredibly complicated Architecture. To those looking for a deeper dive into some of the concepts, models and techniques on which FreeVC is built, there will be some links to further reading.\n",
    "\n",
    "**FreeVC: [Paper](https://arxiv.org/abs/2210.15418) | [Demo](https://olawod.github.io/FreeVC-demo/) | [Code](https://github.com/OlaWod/FreeVC)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "### What is Voice Conversion?\n",
    "\n",
    "Voice conversion is probably best explained using an example: There are two people, Alice and Bob. Alice wants to impersonate Bob, and she has a recording of Bob's voice. Alice then makes recordings of herself, saying things Bob would never say, such as \"give lots of money to Alice\". Using the recording she has of Bob, she *converts* the voice to sounding like Bob is saying all these ridiculous things.\n",
    "\n",
    "In this scenario, Alice is the **source** and Bob is the **target voice**. The distinction may seem a bit arbitrary at first, since we need both a recording of Alice *and* Bob, but since Bob's voice is where we want to end up, this distinction makes sense.\n",
    "\n",
    "#### Intuition\n",
    "Without going into technical details, let's ask why this works *intuitively*.\n",
    "\n",
    "When we listen to a voice, our brains more or less automatically process two different things:\n",
    "- *Who is speaking?* (let's call that **identity** or **speaker information**)\n",
    "- *What is being said?* (let's call that **content**)\n",
    "Determining the identity comes down to factors both physically inherent to your voice - mainly timbre and pitch - as well as factors that are more under the speaker's direct control, things like accent, rhythm.\n",
    "\n",
    "The content is largely independent of the features that make up identity. Communication works because people with different voices are able to produce the same phonemes. The same sequence of vowels and consonants - be it a word, sentence or speech - means the same thing across different speakers.\n",
    "\n",
    "The identity and content information on a *signal* (a spoken utterance) seem to be independent of each other, to a certain degree. The core idea for voice conversion therefore is the following:\n",
    "1. Given a source signal (by speaker A), strip it all its speaker information, while preserving content. \n",
    "2. Extract target speaker information (speaker B).\n",
    "3. Insert target speaker information into the stripped source signal\n",
    "\n",
    "<details>\n",
    "\n",
    "**<summary>Difference to Voice Cloning</summary>**\n",
    "\n",
    "How does voice conversion differ from voice cloning?\n",
    "\n",
    "Fundamentally, voice cloning - the extracting of speaker features and using them to generate speech - falls in the realm of text-to-speech, whilst voice conversion is speech-to-speech and entirely textless. There are also some significant practical differences: For instance, sophisticated voice cloning models also preserve properties such as accent and rhythm, whereas voice conversion does not. The stripped signal in voice conversion is much more of a rigid template than the text input used in voice cloning. The advantage of this is that it allows the user more direct control over the rhythm, accent, pitch contour and so on, simply by having the desired patterns in the source signal.\n",
    "\n",
    "</details>\n",
    "\n",
    "### FreeVC: Architecture\n",
    "\n",
    "A significant part of FreeVC's architecture is based on [**VITS**](https://github.com/jaywalnut310/vits), an **end-to-end text-to-speech** model. VITS is a popular model because its output sounds very human for a TTS system. A core piece of VITS is the [Conditional Variational Autoencoder](https://theaiacademy.blogspot.com/2020/05/understanding-conditional-variational.html), a type of autoencoder more suitable for d\n",
    " However, as a TTS system it uses text as the input, whereas voice conversion is a speech-to-speech task. Roughly summarized and glossing over many technical details, FreeVC modifies the architecture of VITS in two ways: \n",
    "\n",
    "Firstly, it replaces the text encoder with an encoder capable of handling speech. The encoder itself consists of multiple pieces: A pretrained model that transforms the raw waveform into a vector (WavLM), a *bottleneck extractor* that reduces the dimensionality of the obtained vector and hopefully sieves out the information not needed, and lastly a normalizing flow (read more on flow [here](https://towardsdatascience.com/introduction-to-normalizing-flows-d002af262a4b)).\n",
    "\n",
    "Secondly, it adds a pretrained **speaker encoder** to the model architecture. A mel-spectrogram of the audio sample of the target speaker is given as input to the speaker encoder. It extracts the relevant speaker features, and feeds the resulting *speaker embedding* into both the flow module and the decoder.\n",
    "\n",
    "Finally, the decoder takes the encoded source audio and the speaker embedding, and creates the output waveform from it. As in VITS, FreeVC also uses [HiFi-GAN V1](https://github.com/jik876/hifi-gan) as its decoder.\n",
    "\n",
    "Additionally, FreeVC uses a discriminator to incorporate [adversarial learning](https://developers.google.com/machine-learning/gan/) as well as data augmentation during training of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### Setup Venv & install requirements\n",
    "\n",
    "#### Ensure Prerequisites2\n",
    "\n",
    "Firstly, make sure that Python 3.9 and FFmpeg are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Python 3.9\n",
    "! python3.9 -V\n",
    "# Check for FFmpeg\n",
    "! ffmpeg -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "**<summary>Install Python 3.9 & FFmpeg</summary>**\n",
    "\n",
    "If checking with the above commands gives you an error, follow these steps:\n",
    "\n",
    "##### Python 3.9\n",
    "```bash\n",
    "sudo add-apt-repository ppa:deadsnakes/ppa\n",
    "sudo apt-get update && sudo apt upgrade\n",
    "sudo apt-get install python3.9\n",
    "```\n",
    "\n",
    "##### FFmpeg\n",
    "```bash\n",
    "sudo apt update\n",
    "sudo apt install ffmpeg\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Venv & Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate venv\n",
    "! source .venv-freevc/bin/activate\n",
    "# install requirements\n",
    "! pip install -r requirements.txt\n",
    "## add the venv to the registry of jupyter kernels, allowing us to use  allows jupyter \n",
    "## Seemingly not necessary, but left in commented just in case\n",
    "# ! python3.9 -m ipykernel install --name=.venv-freevc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WavLM\n",
    "\n",
    "&#x2757; **Download** the `WavLM Large` model found on [this page](https://github.com/microsoft/unilm/tree/master/wavlm).\n",
    "> `Pre-Trained Models` > WavLM Large `Google Drive`\n",
    "\n",
    "Next, move the downloaded file into the `wavlm/` folder.\n",
    "\n",
    "You'll want to end up with the following folder structure:\n",
    "> ```ascii\n",
    "> FreeVC-finetune/\n",
    "> ├─ ...\n",
    "> ├─ wavlm/\n",
    "> │  ├─ modules.py/\n",
    "> │  ├─ WavLM-Large.pt\n",
    "> │  ├─ WavLM-Large.pt.txt\n",
    "> │  ├─ WavLM.py\n",
    "> │  ├─ __init__.py\n",
    "> ├─ ...\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HiFi-GAN\n",
    "\n",
    "&#x2757; **Download** the HiFi-GAN model `VCTK_V1` found on [this page](https://github.com/jik876/hifi-gan?tab=readme-ov-file). \n",
    "> `Download Pretrained Models` > Google Drive: `VCTK_V1` > Download `generator_v1`\n",
    "\n",
    "Next, move the downloaded file into the `hifigan/` folder.\n",
    "\n",
    "You'll want to end up with the following folder structure:\n",
    "> ```ascii\n",
    "> FreeVC-finetune/\n",
    "> ├─ ...\n",
    "> ├─ hifigan/\n",
    "> │  ├─ __init__.py\n",
    "> │  ├─ config.json\n",
    "> │  ├─ generator_v1\n",
    "> │  ├─ generator_v1.txt\n",
    "> │  ├─ models.py/\n",
    "> ├─ ...\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the user followed the previous two steps correctly\n",
    "import os\n",
    "file = \"wavlm/WavLM-Large.pt\"\n",
    "assert os.path.exists(file), f\"{file} is missing.\\nMake sure you downloaded the WavLM-Large model and put it in the correct directory\"\n",
    "file = \"hifigan/generator_v1\"\n",
    "assert os.path.exists(file), f\"{file} is missing.\\nMake sure you downloaded the HiFi-Gan model (VCTK_V1) and put it in the correct directory\"\n",
    "\n",
    "# already creating a folder required for the next step if it doesn't exist yet\n",
    "if not os.path.isdir(\"./checkpoints\"):\n",
    "    os.mkdir(\"./checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FreeVC checkpoints\n",
    "\n",
    "&#x2757; To fine-tune a base model, we'll also need to **download** the checkpoints of the base model. These should be able to be found under [this link](https://1drv.ms/u/s!AnvukVnlQ3ZTx1rjrOZ2abCwuBAh?e=UlhRR5). If the link does no longer work, check the README in the [original FreeVC repository](https://github.com/OlaWod/FreeVC) and try the link provided in the paragraph that reads \"_We also provide the pretrained models_\".\n",
    "\n",
    "This link should take you to a OneDrive folder that contains the checkpoint. We won't need all of the files there, only two of them:\n",
    "- `D-freevc.pth`\n",
    "- `freevc.pth`\n",
    "\n",
    "We can also ignore the folder `24kHz` entirely, as we're only working with the 16kHz models and audio.\n",
    "\n",
    "Move the two checkpoints to the `checkpoints/` folder.\n",
    "\n",
    "You'll want to end up with the following folder structure:\n",
    "> ```ascii\n",
    "> FreeVC-finetune/\n",
    "> ├─ ...\n",
    "> ├─ checkpoints/\n",
    "> │  ├─ D-freevc.pth\n",
    "> │  ├─ freevc.pth\n",
    "> ├─ ...\n",
    "> ```\n",
    "\n",
    "><details>\n",
    ">\n",
    ">**<summary> &#x2753; Why two checkpoints?</summary>**\n",
    ">\n",
    ">FreeVC trains two different models because of it is, in part, a generative adversarial network (GAN). `freevc.pth` is the generator net, the part of the model that actually generates the voice-converted audio. `D-freevc.pth` is the checkpoint of the discriminator, the net that tries to classify an audio as  as either 'natural' (i.e. a real recording of a human, not generated by the model we are training) or 'generated'. It takes both generated and natural audio as inputs for its training. When classifying generated audio, the discriminator's performance feeds into the loss function of the generator. Intuitively, the generator's weight receive stronger updates if it manages to 'fool' the discriminator. Thus, if the generator gets better, the discriminator is forced to improve to keep up with it, and vice versa. Since these are essentially two different models, we'll need to load them separately.\n",
    ">\n",
    ">For inference (= just converting audio, no training), we only require the generator.\n",
    ">\n",
    "></details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the checkpoints are downloaded and stored in the right directories.\n",
    "import os\n",
    "files = [\"./checkpoints/D-freevc.pth\", \"./checkpoints/freevc.pth\"]\n",
    "for file in files:\n",
    "    assert os.path.exists(file), f\"{file} is missing.\\nMake sure you downloaded the WavLM-Large model and put it in the correct directory\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Preparation\n",
    "\n",
    "To finetune the pre-trained model, we of course need some training data to adapt the model to the target speaker. With _some_ training data, I mean a whole lot of it.\n",
    "\n",
    "We want to make sure we make the most of our data. Therefore, we'll do some simple preprocessing on it.\n",
    "\n",
    "#### Chop it up\n",
    "\n",
    "The **base model** (i.e. FreeVC's pretrained model that we're finetuning) is trained on the [VCTK corpus](https://datashare.ed.ac.uk/handle/10283/3443). The audio in this corpus is stored in multiple smaller files (3.4 seconds on average) than one large file.\n",
    "\n",
    "If your finetuning-data is already in similarly sized chunks, you can **skip this step**. Otherwise, run the following cells on your file(s): This will automatically detect silent passages - for example between sentences, and thus split your audio into adequately sized chunks.\n",
    "\n",
    "You can modify the following parameters to control the chunking:\n",
    "- `MIN_SILENCE_LEN` (miliseconds): Defines the minimal length of silence necessary to split the audio at that point\n",
    "- `SILENCE_THRESHOLD` (dBFS): Defines what counts as silent and what does not; anything louder than the set threshold will count as not silent. \n",
    "- `MIN_CHUNK_LEN` (seconds): Any chunk shorter than this value will be discarded and NOT saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified code, originally from:\n",
    "#   https://www.codespeedy.com/split-audio-files-using-silence-detection-in-python/\n",
    "#   retrieved on 2024-08-23\n",
    "import os\n",
    "import copy\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "from typing import Union\n",
    "\n",
    "def chunk_audio(filelist: list[str], silence_len=800,silence_thr=-40, chunklen: float=0., training_len: int=300, out_path=\"./chunks\", quiet: bool=False):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    ---\n",
    "        filelist: list of files (relative paths required) to be used as training data\\\\\n",
    "        silence_len: define how long a silent period needs to be for a split to occur (miliseconds)\\\\\n",
    "        silence_thr: set the intensity threshold, values below it will be counted as silence (dBFS)\\\\\n",
    "        chunklen: required mimimum length of a chunk for it to be saved\\\\\n",
    "        training_len: intended length of the training data (seconds)\\\\\n",
    "        out_path: where to store the resulting audio chunks\\\\\n",
    "        quiet: whether or not to print any status messages\\\\\n",
    "    \"\"\"\n",
    "    # necessary to avoid outside-scope filelist being emptied inside current function\n",
    "    filelist = copy.copy(filelist)\n",
    "    count = 0\n",
    "    length = 0.\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "    while length <= training_len and filelist != []:\n",
    "        for file in filelist:\n",
    "            filelist.remove(file)\n",
    "            # load file (may take long for large files)\n",
    "            sound = AudioSegment.from_wav(file)\n",
    "            # spliting audio files\n",
    "            audio_chunks = split_on_silence(sound, min_silence_len=silence_len, silence_thresh=silence_thr)\n",
    "            #loop is used to iterate over the output list\n",
    "            for pre_chunk in audio_chunks:\n",
    "                # save them as a FLAC file\n",
    "                cut_chunks = _cut_chunks(pre_chunk)\n",
    "                for chunk in cut_chunks:\n",
    "                    if chunk.duration_seconds >= chunklen:\n",
    "                        output_file = \"{0}/chunk{1}.flac\".format(out_path, count+1)\n",
    "                        # if the current chunk will exceed the intended length of the training data,\n",
    "                        # cut it in order to exactly reach the training length\n",
    "                        if length+chunk.duration_seconds >= training_len:\n",
    "                            overlength = (length+chunk.duration_seconds)-training_len\n",
    "                            overlength_ms = round(overlength*1000)\n",
    "                            chunk = chunk[:-overlength_ms]\n",
    "                        if chunk.duration_seconds == 0: continue\n",
    "                        length += chunk.duration_seconds\n",
    "                        count += 1\n",
    "                        chunk.export(output_file, format=\"flac\")\n",
    "                        # skip printing if quiet-flag is set (exists mostly for not cluttering the testing)\n",
    "                        if quiet: continue\n",
    "                        print(\"Exported file\", output_file, \"({0})\".format(len(chunk)))\n",
    "                    else:\n",
    "                        if quiet: continue\n",
    "                        print(\"Skipping Chunk: Too short (< {0} seconds)\".format(chunklen))\n",
    "    if not quiet:\n",
    "        print(\"\\nAverage length of saved chunks: {0} Seconds\".format(round(length/count,2)))\n",
    "        print(\"\\nTotal length of saved chunks: {0} Seconds\".format(round(length,2)))\n",
    "\n",
    "\n",
    "def _cut_chunks(chunk: AudioSegment):\n",
    "    out_list = []\n",
    "    if chunk.duration_seconds >= 4:\n",
    "        total_len = len(chunk)\n",
    "        half_len = total_len // 2\n",
    "        new_chunks = [chunk[:half_len], chunk[half_len:]]\n",
    "        for c in new_chunks:\n",
    "            out_list.extend(_cut_chunks(c))\n",
    "        return out_list\n",
    "    else:\n",
    "        return [chunk]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unittest of function chunk_audio(), with setup/teardown fixtures\n",
    "#   Testing is limited to a single set of parameters.\n",
    "import unittest\n",
    "import os\n",
    "import shutil\n",
    "from pydub import AudioSegment\n",
    "\n",
    "class TestChunkAudio(unittest.TestCase):\n",
    "    pass\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        cls.out_path = \"./test/resources/temp\"\n",
    "        os.mkdir(cls.out_path)\n",
    "        chunk_audio(filelist=[\"./test/resources/test_audio_12s.wav\"], silence_len=800, silence_thr=-40, chunklen=1.5, training_len=6, out_path=cls.out_path, quiet=True)\n",
    "\n",
    "    def test_number_of_files(self):\n",
    "        # check number of generated files against expected number\n",
    "        self.assertEqual(len(os.listdir(self.out_path)), 3)\n",
    "\n",
    "    def test_total_length(self):\n",
    "        # sum length of chunks...\n",
    "        total_len = sum([AudioSegment.from_file(f'{self.out_path}/{file}', format=\"flac\").duration_seconds for file in os.listdir(self.out_path)])\n",
    "        # ...check against expected length. because we're dealingfloating point numbers, we may not get perfectly round numbers, thus we only check near-equality.\n",
    "        self.assertAlmostEqual(total_len,6.0, 4)\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        shutil.rmtree(cls.out_path)\n",
    "        # for file in os.listdir(cls.out_path):\n",
    "\n",
    "res = unittest.main(argv=[''], verbosity=3, exit=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES\n",
    "MIN_SILENCE_LEN = 800\n",
    "SILENCE_THRESHOLD = -40\n",
    "MIN_CHUNK_LEN = 1.5\n",
    "\n",
    "# USER TODO: list of files to chunk\n",
    "filelist = []\n",
    "assert filelist != []\n",
    "\n",
    "chunk_audio(filelist, silence_len=MIN_SILENCE_LEN, silence_thr=SILENCE_THRESHOLD, chunklen=MIN_CHUNK_LEN, training_len=6, out_path=\"./z_testchunks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#x270f;&#xfe0f; Insert the relative path to the audio-files of your target speaker into `filelist`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES\n",
    "MIN_SILENCE_LEN = 800\n",
    "SILENCE_THRESHOLD = -40\n",
    "MIN_CHUNK_LEN = 1.5\n",
    "\n",
    "# USER TODO: list of files to use for training\n",
    "filelist = [\"\"]\n",
    "\n",
    "for file in filelist:\n",
    "    assert os.path.exists(file), f\"{file} is not a valid file path. Make sure you are using each file's relative path (from FreeVC-finetune)\"\n",
    "\n",
    "chunk_audio(filelist, silence_len=MIN_SILENCE_LEN, silence_thr=SILENCE_THRESHOLD, chunklen=MIN_CHUNK_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "At this point, we have some audio data in appropriately sized chunks. We now need to run some very particular preprocessing steps on it, so that the model receives it in the right format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storage Format\n",
    "\n",
    "FreeVC expects our fine-tuning data to be in the same format as its original training data - the [VCTK-dataset](https://datashare.ed.ac.uk/handle/10283/3443) . Therefore, we need to rename some files and move them to the right places before we run any preprocessing.\n",
    "\n",
    "You'll need to assign some **4-character** ID to your speaker - pick one that makes sense to you. If it's longer or shorter than 4 characters, this won't work.\n",
    "\n",
    "&#x270f;&#xfe0f; Add the ID of your choice under `SPEAKER_ID`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER TODO:  pick your Speaker ID\n",
    "SPEAKER_ID = \"xxxx\"\n",
    "\n",
    "assert len(SPEAKER_ID) == 4\n",
    "\n",
    "# DIRECTORY NAMES\n",
    "CHUNKS = \"./chunks/\"\n",
    "FLACS = \"./dataset/flac/\"\n",
    "REL_DATA_PATH = f'{FLACS}{SPEAKER_ID}/'\n",
    "DATA16K = \"dataset/finetuning-16k\"\n",
    "DATA22K = \"dataset/finetuning-22k\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will rename your audio and move it into a directory with the right structure.\n",
    "\n",
    "`<some_dir>/<sp_id>/<sp_id-filename>_mic2.flac`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "if not os.path.exists(REL_DATA_PATH):\n",
    "    os.makedirs(REL_DATA_PATH)\n",
    "\n",
    "for i,file in enumerate(os.listdir(CHUNKS)):\n",
    "    # rename & move files to the specific format necessary\n",
    "    new_filename = f'{SPEAKER_ID}-{i}_mic2.flac'\n",
    "    a =os.path.join(CHUNKS,file)\n",
    "    shutil.copy(a, os.path.join(REL_DATA_PATH,new_filename))\n",
    "print(\"Moved and renamed your training files.\\nGreat Success!! Very Nice!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downsampling\n",
    "\n",
    "Downsamples the audio to 16kHz.\n",
    "- `--sr1` sampling rate`\n",
    "- `--sr2` sampling rate`\n",
    "- `--in_dir` path to source dir`\n",
    "- `--out_dir1` path to target dir`\n",
    "- `--out_dir2` path to target dir`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python downsample.py --in_dir $FLACS --out_dir1 $DATA16K --out_dir2 $DATA22K\n",
    "! ln -s $DATA16K DUMMY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Splitting\n",
    "\n",
    "Next, our fine-tuning data will need to be split into a training, test and validation set.\n",
    "\n",
    "The original splitting-script of FreeVC uses 2 chunks from each speaker for validation, 10 chunks for testing and the rest for training. With an average of around 400 chunks per speaker, this is an average test-split of 2.5%, and validation-split of 0.5%. To me, this seems like an overly small test and validation portion.\n",
    "\n",
    "Therefore the preprocessing script was modified:\n",
    "Before, the test and validation portions were constant, at 10 and 2 samples respectively. I changed them to a relative 5% and 1% portion for the test and validation sets.\n",
    "\n",
    "_(As to whether this improves or worsens performance, I have no empirical evidence for either and I do not intend to gather it.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_file=\"./filelists/finetune-val.txt\"\n",
    "test_file=\"./filelists/finetune-test.txt\"\n",
    "train_file=\"./filelists/finetune-train.txt\"\n",
    "\n",
    "! python preprocess_flist.py --train_list $train_file --test_list $test_file --val_list $val_file --source_dir $DATA16K\n",
    "! rm DUMMY\n",
    "! ln -s $DATA16K DUMMY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speaker Encoder (pretrained)\n",
    "\n",
    "Something something encode speaker information using a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare variable\n",
    "DATA_ROOT=\"./dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "! CUDA_VISIBLE_DEVICES=0 python preprocess_spk.py --in_dir $DATA16K --out_dir_root $DATA_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Augmentation\n",
    "\n",
    "To make the most of our data...    ...Spectrogram Resize (SR)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare variable\n",
    "HIFIGAN_CFG = \"hifigan/config.json\"\n",
    "WAV_DIR = \"dataset/sr/wav\"\n",
    "SSL_DIR = \"dataset/sr/wavlm\"\n",
    "\n",
    "# Perform data augmentation (spectrogram resize)\n",
    "! CUDA_VISIBLE_DEVICES=0 python preprocess_sr.py --in_dir $DATA22K --wav_dir $WAV_DIR --ssl_dir $SSL_DIR --config $HIFIGAN_CFG --min 68 --max 92 --sr 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning\n",
    "\n",
    "The hyperparameters of training are set in a JSON file, located in the `/configs/` directory. For finetuning, we'll use the file `freevc-finetune.json`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train freevc: use config 'configs/freevc-finetune.json', use model 'freevc'\n",
    "MODEL_NAME = f'freevc_finetune-{SPEAKER_ID}'\n",
    "MODEL_NAME = f'freevc_finetune'\n",
    "! echo $MODEL_NAME\n",
    "! CUDA_VISIBLE_DEVICES=0 python finetune.py -c configs/freevc-finetune.json -m $MODEL_NAME -d ./checkpoints/D-freevc.pth -g ./checkpoints/freevc.pth --force_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Output\n",
    "\n",
    "We're almost there.\n",
    "\n",
    "As a last step, we'll need to define the audio recording(s) that we actually want to convert to our target speakers. To do so, all we need to do is edit the file `convert.txt` - let's call it the **task file**. \n",
    "\n",
    "The structure of the task file is simple: \n",
    "- Each row corresponds to a single *task*, i.e. one source audio being converted to a target speaker.\n",
    "- There are three columns for each row, separated by a single pipe symbol (`|`). The first column defines the name of the task, the second column contains the path to the source file, and the third column contains the path to an audio file of the target speaker.\n",
    "\n",
    "This could look as follows: (Alice is the source, Bob the target speaker.)\n",
    "\n",
    "```txt\n",
    "        alice2bob_1|PATH/TO/ALICE_1.wav|PATH/TO/BOB.wav\n",
    "        alice2bob_2|PATH/TO/ALICE_2.wav|PATH/TO/BOB.wav\n",
    "        alice2bob_3|PATH/TO/ALICE_3.wav|PATH/TO/BOB.wav\n",
    "        ...\n",
    "```\n",
    "\n",
    "- `alice2bob_X` is simply the name of the conversion - this will mainly be used to name the output file and in the logs.\n",
    "- `PATH/TO/ALICE_X.wav` is the path to the source files - recordings of Alice, which we want to convert to Bob's voice\n",
    "- `PATH/TO/BOB.wav` is the path to the audiofile of Bob's voice - the target. Note that this can be the same for various different source files.\n",
    "\n",
    "> **Tip &#x1F4A1;**\n",
    ">\n",
    "> Within the task file, we do **not** need to adhere to specific filenaming (such as 4-character speaker ids) \n",
    "\n",
    "Each user's task file will look different. Thus, this is something you'll have to do yourself. To help you however, there's a simple function (`fill_task_file()`) to potentially make things a bit easier and faster. Note that it requires you to have all your source files in the same directory, and the name will be kept relatively simple. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_task_file(basename: str, source: str, target: str, taskfile: str=\"convert.txt\") -> str:\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    ---\n",
    "        basename: Name to be used as the base for naming each conversion. The name of the nth task will be 'basename_n'.\\\\\n",
    "        source: path to the DIRECTORY containing the source files.\\\\\n",
    "        target: path to the FILE containing the target speaker.\\\\\n",
    "        taskfile: allows the user to name their task file something other than 'convert.txt'.\\\\\n",
    "    \n",
    "    Output:\n",
    "    ---\n",
    "        taskfile\n",
    "    \"\"\"\n",
    "    # clear file\n",
    "    with open(taskfile, \"w\", encoding=\"utf-8\") as f: f.write()\n",
    "    # fill file\n",
    "    with open(taskfile, \"a\", encoding=\"utf-8\") as f:\n",
    "        for i,file in enumerate(os.listdir(source)):\n",
    "            f.write(f\"{basename}_{i}|{file}|{target}\")\n",
    "    return taskfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "&#x270F;&#xFE0F;\n",
    "Enter the desired conversions into `convert.txt`. You can use the function `fill_task_file()` to do this in a quick and simple way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER TODO: adjust parameters\n",
    "converttxt = fill_task_file(\"alice2bob\", source=\"PATH/TO/DIR/ALICE\", target=\"PATH/TO/FILE/BOB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util_finetune import get_max_checkpoint\n",
    "\n",
    "checkpoint = get_max_checkpoint(MODEL_NAME)\n",
    "\n",
    "# convert the audio to the target speaker, according to the convert.txt\n",
    "! CUDA_VISIBLE_DEVICES=0 python convert.py --hpfile configs/freevc-finetune.json --ptfile $checkpoint --txtpath convert.txt --outdir outputs/freevc-finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The converted audio should now appear in the folder you specified as the output directory - go check it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running multiple fine-tuning experiments\n",
    "\n",
    "Perhaps you now want to experiment with different settings. The function `wrap_experiments()` does takes two lists as input arguments (among others):\n",
    "- `training_amounts` : a list of different amounts of training data (in seconds)\n",
    "- `different_epochs` : a list with varying number of epochs\n",
    "\n",
    "When the function is run, each different training-amount is paired with each number of epochs, and a model is fine-tuned according to these two parameters (whilst the remaining parameters stay the same for all combinations).\n",
    "\n",
    "The experiment-wrapper makes sure you have to only run the necessary pre-processing steps for each training amount once. After the pre-processing all the audio for a specific training amount, it fine-tunes separate models on the same data but with different numbers of epochs. Additionally, it provides an option to **keep the training data stored** even after a run is finished by setting the `keep_data`-flag to `True`, This way, in case of subsequent fine-tuning runs with the same amount of training data, the pre-processing steps can be skipped. Instead, the data is simply moved from its depot into the active directories used during training.\n",
    "\n",
    "It finishes each fine-tuning run by also generating audio -- again using the 'tasks' in `convert.txt`.\n",
    "\n",
    "Model checkpoints and generated outputs are named according to the following format: <br>`freevc-ft-<SPEAKER_ID>-<TRAINING_AMOUNT>s-<EPOCHS>ep`\n",
    "\n",
    "\n",
    "<details>\n",
    "\n",
    "**<summary>Example</summary>**\n",
    "\n",
    "Let's say you wanna compare fine-tuning on 30 seconds and 5 minutes of training data, as well as 2 and 10 epochs.\n",
    "\n",
    "The experiment wrapper trains a total of $4$ ($2\\times 2$) fine-tuned models, in all possible combinations: \n",
    "- 30 seconds, 2 epochs\n",
    "- 30 seconds, 10 epochs\n",
    "- 300 seconds, 2 epochs \n",
    "- 300 seconds, 10 epochs \n",
    "\n",
    "It would first generate and pre-process the 30-second training data, create a fine-tuned model on 2 and 10 epochs respectively, and then repeat the process for the 300-second training data.\n",
    "\n",
    "The two input lists in this example would look as follows:\n",
    "```python\n",
    "amounts = [30, 300]\n",
    "epochs = [2, 10]\n",
    "\n",
    "wrap_experiments(training_amounts=amounts, different_epochs=epochs, ...)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from util_finetune import get_max_checkpoint\n",
    "\n",
    "# function used to wrap a single experiment, wraps several audio processing functions, training and conversion\n",
    "def train_and_generate(audio_len: int=300, epochs: int=5, new_config_pars: dict={}, speaker_id: str=\"\", force_train: bool=True):\n",
    "    \"\"\"\n",
    "    Fine-tune a single model with a certain amount of training data on a certain number of epochs.\n",
    "    \n",
    "    Inputs\n",
    "    ---\n",
    "        **audio_len**: length of audio used for training in seconds. If the value exceeds the maximum possible value of the training data, it defaults to the maximum possible value\\\\\n",
    "        epochs: number of epochs used for finetuning\\\\\n",
    "        new_config_pars: dict containing some specific parameters for training, mainly the required training files at this point\\\\\n",
    "        speaker_id: 4-character speaker ID.\\\\\n",
    "        force_train: flag indicating whether or not to overwrite an existing fine-tuned model, or to continue fine-tuning from an already existing (fine-tuned) model\\\\\n",
    "        keep_data: flag that determines if the training data is kept or deleted. Includes everything from WAVs, FLACs, mel-spectrograms to speaker embeddings\\\\\n",
    "    Returns\n",
    "    ---\n",
    "\n",
    "    \"\"\"\n",
    "    # check that the following variables are indeed declared, by asserting that they aren't the default values.\n",
    "    #   You may ask, \"why have default values then, if you don't actually want those values?\".\n",
    "    #   Well, because I want to have them in that position and I have some default values declared before, \n",
    "    #   and python doesn't let me have any parameters without default values later.\n",
    "    #   Is it \"nice\" programming style? Probably not. Do I care? Not enough. Does it matter? Not really. Is this comment getting way too long? Yes.\n",
    "    assert speaker_id != \"\"\n",
    "    assert new_config_pars != {}\n",
    "\n",
    "    # make changes to a copy of the finetune-config, leaving the original untouched\n",
    "    config_file = shutil.copy(\"./configs/freevc-finetune.json\", \"./configs/freevc-finetune-exp.json\")\n",
    "\n",
    "    if not os.path.exists(config_file): pass\n",
    "    \n",
    "    MODEL_NAME = f\"freevc-ft-{speaker_id}-{audio_len}s-{epochs}ep\"\n",
    "\n",
    "    if not os.path.exists(f'./checkpoints/{MODEL_NAME}') or force_train:\n",
    "        with open(config_file, \"r\", encoding=\"utf-8\") as cfg:\n",
    "            config = json.load(cfg)\n",
    "        # modify config file (set epochs and filelists)\n",
    "        config[\"train\"][\"epochs\"] = epochs\n",
    "        config[\"train\"][\"eval_interval\"] = epochs\n",
    "        config[\"train\"][\"log_interval\"] = epochs\n",
    "        config[\"data\"][\"training_files\"] = new_config_pars[\"training_files\"]\n",
    "        config[\"data\"][\"validation_files\"] = new_config_pars[\"validation_files\"]\n",
    "        \n",
    "        with open(config_file, \"w\", encoding=\"utf-8\") as cfg:\n",
    "            json.dump(config, cfg, indent=4)\n",
    "\n",
    "        ! CUDA_VISIBLE_DEVICES=0 python finetune.py -c configs/freevc-finetune-exp.json -m $MODEL_NAME -d ./checkpoints/D-freevc.pth -g ./checkpoints/freevc.pth --force_new\n",
    "\n",
    "    checkpoint = get_max_checkpoint(modelname=MODEL_NAME)\n",
    "    converttxt = \"convert.txt\"\n",
    "    ! CUDA_VISIBLE_DEVICES=0 python convert.py --hpfile configs/freevc-finetune-exp.json --ptfile $checkpoint --txtpath $converttxt --outdir outputs/$MODEL_NAME\n",
    "\n",
    "    return MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "from util_finetune import move_all_files\n",
    "\n",
    "def wrap_experiments(training_amounts: list[int], different_epochs: list[int], training_data: list[str], speaker_id: str, keep_stored: bool=True, force_train: bool=True):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    ---\n",
    "        **training_amounts**: list of different amounts of training data (i.e. length of the audio training material, (in seconds))\\\\\n",
    "        **different_epochs**: list of different amounts of epochs to finetune the model on\\\\\n",
    "        **training_data**: list of paths of all files to be considered for training\\\\\n",
    "        **speaker_id**: 4-symbol ID of the speaker\\\\ \n",
    "        **keep_stored**: for different amounts of training data, instead of deleting the data simply \"archive it\" so it does not have to be regenerated in future experiments with the same amount.\\\\\n",
    "        **force_train**: if a finetuning configuration already exists, don't train, only convert\\\\\n",
    "    \"\"\"\n",
    "    # speaker_id is needed to name/identify training data \n",
    "    assert speaker_id != \"\"\n",
    "    assert os.listdir(\"./\")\n",
    "    \n",
    "    # list of models (strings of identifying model names) that will be the output of the experiment wrapper\n",
    "    finetuned_models = []\n",
    "\n",
    "    if not os.path.exists(\"./dataset/\"):\n",
    "        os.mkdir(\"./dataset\")\n",
    "    # idea: iterate through audio lengths first (all of which share the same training data).\n",
    "    #   allows us to keep the training data whilst experimenting with different number of epochs\n",
    "    for tl in training_amounts:\n",
    "        try:\n",
    "            # consistent naming scheme including speaker & amount of training data, used to keep training files stored (in ./depot/...)\n",
    "            depot_path = f\"./depot/{speaker_id}/{str(tl)}_sec\"\n",
    "\n",
    "            # adjust paths ONLY IF NECESSARY\n",
    "            DATA16K =  \"./dataset/finetuning-16k\"\n",
    "            CHUNKS =  \"./chunks/\"\n",
    "            DATA_ROOT =  \"./dataset\"\n",
    "            \n",
    "            training_amt = f'{tl}sec'\n",
    "            val_file= f\"./filelists/finetune-{training_amt}-val.txt\"\n",
    "            test_file =  f\"./filelists/finetune-{training_amt}-test.txt\"\n",
    "            train_file =  f\"./filelists/finetune-{training_amt}-train.txt\"\n",
    "            \n",
    "            # create directories if they do not exist yet\n",
    "            for dir in [CHUNKS, \"./filelists\"]:\n",
    "                if not os.path.exists(dir):\n",
    "                    os.mkdir(dir)\n",
    "\n",
    "            if os.path.exists(depot_path) and os.listdir(depot_path):\n",
    "                print(\"path exists, moving files from depot to active directory...\")\n",
    "                # \"recover\" existing data from the depot, avoid recreating data that already exists\n",
    "                #   data (./dataset)\n",
    "                move_all_files(src=f'{depot_path}/dataset', dst=\"./dataset\")\n",
    "                #   chunks\n",
    "                move_all_files(src=f'{depot_path}/chunks', dst=CHUNKS)\n",
    "                #   filelists\n",
    "                move_all_files(src=f'{depot_path}/filelists', dst=\"./filelists\")\n",
    "                #   create DUMMY-link\n",
    "                ! ln -s $DATA16K DUMMY\n",
    "\n",
    "            # OR call audio pre-processing for complete training amount\n",
    "            else:\n",
    "                print(f'\\nPROCESSING AUDIO:\\n\\t Amount of Training Data: {tl}s')\n",
    "                # create chunks\n",
    "                chunk_audio(filelist=training_data, training_len=tl)\n",
    "\n",
    "                # declare variables necessary for DOWNSAMPLING\n",
    "                FLACS =  \"./dataset/flac/\"\n",
    "                REL_DATA_PATH =  f'{FLACS}{speaker_id}/'\n",
    "                DATA22K =  \"./dataset/finetuning-22k\"\n",
    "\n",
    "                # create directories where necessary\n",
    "                for dir in [FLACS, REL_DATA_PATH, DATA22K]:\n",
    "                    if not os.path.exists(dir):\n",
    "                        os.mkdir(dir)\n",
    "                \n",
    "                for i,file in enumerate(os.listdir(CHUNKS)):\n",
    "                    # rename & move files to the specific format necessary\n",
    "                    new_filename = f'{SPEAKER_ID}-{i}_mic2.flac'\n",
    "                    a =os.path.join(CHUNKS,file)\n",
    "                    shutil.copy(a, os.path.join(REL_DATA_PATH,new_filename))\n",
    "                # downsampling operation\n",
    "                print(\"Moved and renamed your training files.\\nGreat Success!! Very Nice!\")\n",
    "                ! python downsample.py --in_dir $FLACS --out_dir1 $DATA16K --out_dir2 $DATA22K\n",
    "\n",
    "\n",
    "                # create filelists\n",
    "                ! python preprocess_flist.py --train_list $train_file --test_list $test_file --val_list $val_file --source_dir $DATA16K\n",
    "                ! ln -s $DATA16K DUMMY\n",
    "\n",
    "                # declare variables necessary for SPEAKER ENCODING\n",
    "                ! CUDA_VISIBLE_DEVICES=0 python preprocess_spk.py --in_dir $DATA16K --out_dir_root $DATA_ROOT --num_workers 2\n",
    "\n",
    "                # declare variables necessary for DATA AUGMENTATION\n",
    "                HIFIGAN_CFG =  \"hifigan/config.json\"\n",
    "                WAV_DIR =  \"dataset/sr/wav\"\n",
    "                SSL_DIR =  \"dataset/sr/wavlm\"\n",
    "                # Perform data augmentation (spectrogram resize)\n",
    "                ! CUDA_VISIBLE_DEVICES=0 python preprocess_sr.py --in_dir $DATA22K --wav_dir $WAV_DIR --ssl_dir $SSL_DIR --config $HIFIGAN_CFG --min 68 --max 92 --sr 16000\n",
    "\n",
    "            # make sure filelists actually exist\n",
    "            for file in [val_file, train_file, test_file]:\n",
    "                assert os.path.exists(file), f\"file {file} does not exist.\"\n",
    "\n",
    "            # ### EPOCH LOOP ####\n",
    "            # finetune the model with different numbers of epochs, but using the same amount of training data \n",
    "            for eps in different_epochs:\n",
    "                # adjust config\n",
    "                add_to_config= {\"training_files\": train_file, \"validation_files\": val_file}\n",
    "                \n",
    "                # call training functions\n",
    "                ft_model = train_and_generate(audio_len=tl, epochs=eps, new_config_pars=add_to_config, speaker_id=speaker_id, force_train=force_train)\n",
    "            # ### #### #### ####\n",
    "            finetuned_models.append(ft_model)\n",
    "        finally:\n",
    "            if keep_stored:\n",
    "                # keep files and simply move them to the depot\n",
    "                if not os.path.exists(depot_path):\n",
    "                    os.makedirs(depot_path)\n",
    "\n",
    "                move_all_files(src=\"./dataset\", dst=f'{depot_path}/dataset')\n",
    "                #  chunks\n",
    "                move_all_files(src=\"./chunks\", dst=f'{depot_path}/chunks')\n",
    "                #  filelists\n",
    "                move_all_files(src=\"./filelists\", dst=f'{depot_path}/filelists')\n",
    "                #  DUMMY\n",
    "                ! rm DUMMY\n",
    "                pass\n",
    "            else:\n",
    "                # DELETE existing chunks (./chunks) and data (./dataset)for the current training amount\n",
    "                #  data (./dataset)\n",
    "                shutil.rmtree(\"./dataset\")\n",
    "                #  chunks\n",
    "                # TODO uncomment line\n",
    "                shutil.rmtree(CHUNKS)\n",
    "                #  filelists\n",
    "                for file in [val_file, train_file, test_file]:\n",
    "                    os.remove(file)\n",
    "                # DUMMY (symlink)\n",
    "                ! rm DUMMY\n",
    "    # return a list of modelnames of the finetuned models, so they can be used e.g. to compare generated output\n",
    "    return finetuned_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#x270f;&#xfe0f; Choose or reuse a speaker-ID & insert the relative path to the audio-files of your target speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_amts = [20, 60, 120]\n",
    "epochs = [5,10,20,40]\n",
    "\n",
    "# USER TODO\n",
    "SPEAKER_ID = \"xxxx\"\n",
    "assert len(SPEAKER_ID) == 4\n",
    "\n",
    "# USER TODO\n",
    "training_data = []\n",
    "assert training_data != []\n",
    "\n",
    "models = wrap_experiments(training_amts, epochs, training_data=training_data, speaker_id=SPEAKER_ID, force_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_amts= [60]\n",
    "epochs = [5, 10, 20, 40]\n",
    "\n",
    "# USER TODO\n",
    "SPEAKER_ID = \"xxxx\"\n",
    "assert len(SPEAKER_ID) == 4\n",
    "\n",
    "training_data = []\n",
    "\n",
    "models = wrap_experiments(training_amts, epochs, training_data=training_data, speaker_id=SPEAKER_ID, force_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from util_finetune import get_max_checkpoint\n",
    "\n",
    "def convert_wrapper(models: list[str], task_file=\"convert.txt\", include_base_model: bool=True, output_path: str=\"./outputs\"):\n",
    "    \"\"\"\n",
    "    Wrapper function for generating audio with multiple models.\\\\\n",
    "        Uses each model inside the list `models` to do the voice conversions outlined by the task-file.\\\\\n",
    "        To also do conversions using the base-model (FreeVC pre-finetuning), set the `include_base_model` flag to `True`. Do NOT include the base-model in the model-list.\n",
    "    \"\"\"\n",
    "    assert os.path.isfile(task_file), \"Please pass a valid task_file as an argument.\"\n",
    "    if \"freevc.pth\" in models:\n",
    "        models.remove(\"freevc.pth\")\n",
    "\n",
    "    if include_base_model:\n",
    "        checkpoint = \"./checkpoints/freevc.pth\"\n",
    "        ! CUDA_VISIBLE_DEVICES=0 python convert.py --hpfile configs/freevc-finetune-exp.json --ptfile $checkpoint --txtpath $task_file --outdir $output_path/freevc-base\n",
    "\n",
    "    for model in models:\n",
    "        # skip if base_model, base_model shouldn't be passed into this list.\n",
    "        checkpoint = get_max_checkpoint(modelname=model)\n",
    "        ! CUDA_VISIBLE_DEVICES=0 python convert.py --hpfile configs/freevc-finetune-exp.json --ptfile $checkpoint --txtpath $task_file --outdir $output_path/$model\n",
    "        print(f\"Converted all tasks in {task_file} using the model '{model}'\")\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run additional Conversions (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # do additional conversions with the existing models\n",
    "# models = []\n",
    "# speaker_id = \"brn1\"\n",
    "# epochs = [1,2,5,10]\n",
    "# training_amts = [12,300]\n",
    "# for tl in training_amts:\n",
    "#     for ep in epochs:\n",
    "#         models.append(f\"freevc-ft-{speaker_id}-{tl}s-{ep}ep\")\n",
    "\n",
    "# out_path = \"./outputs\"\n",
    "# models = convert_wrapper(models=models, output_path=out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Display converted audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display converted audio examples\n",
    "import IPython.display as ipd\n",
    "import IPython\n",
    "\n",
    "sr = 16000\n",
    "# where the converted audiofiles are stored \n",
    "out_path = \"./outputs\"\n",
    "\n",
    "print(\"SOURCE AUDIO\")\n",
    "IPython.display.display(ipd.Audio(f'resources/audio/EXAMPLE_SOURCE.wav', rate=sr))\n",
    "print(\"FreeVC base\")\n",
    "IPython.display.display(ipd.Audio(f'{out_path}/freevc-base/EXAMPLE.wav', rate=sr))\n",
    "for mdl in models:\n",
    "    audiofile = f'{out_path}/{mdl}/EXAMPLE.wav'\n",
    "    print(mdl)\n",
    "    IPython.display.display(ipd.Audio(audiofile, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The evaluation of the various models is limited to my personal judgement on outputs generated from a single source- and target speaker, as well as a single example sentence. Based on that, fine-tuning FreeVC with custom data (such as a snippet of Bernie Sander's [famous filibuster](https://commons.wikimedia.org/wiki/File:Bernie_Sanders_-_full_2010-12-10_filibuster.webm), as I did) does not significantly improve the output quality.\n",
    "\n",
    "The effect  of training data - although to be fair, I've only experimented with relatively small amounts (5 minutes at most) -- on the output quality is minimal. Varying the number of epochs has a much larger effect. The challenge here is to find the _soft-spot_ for number of epochs: Too few, and it doesn't sound much closer to the target's voice when compared to the base model. Too many epochs, and the naturalness of the output starts to decline drastically. It sounds more like the target, but it also sounds somewhat robotic and has some strange sounding noise artifacts.\n",
    "\n",
    ">##### Examples:\n",
    ">\n",
    ">**Source speaker**: Micha Hess (author)<br>\n",
    ">**Target speaker**: Bernie Sanders<br>\n",
    ">**Finetuning data amount**: 60 seconds<br>\n",
    ">\n",
    ">\n",
    ">- **Epochs: 5** (`EXAMPLE-brn1-60s-5ep.wav`)\n",
    ">\n",
    "><audio width=\"320\" height=\"160\"\n",
    ">        src=\"./resources/audio/finetuned-results/EXAMPLE-brn1-60s-5ep.wav\"\n",
    ">        controls>\n",
    ">\n",
    ">- **Epochs: 10** (`EXAMPLE-brn1-60s-10ep.wav`)\n",
    ">\n",
    "><audio width=\"320\" height=\"160\"\n",
    ">\t\tsrc=\"./resources/audio/finetuned-results/EXAMPLE-brn1-60s-10ep.wav\"\n",
    ">\t\tcontrols>\n",
    ">\n",
    ">- **Epochs: 20** (`EXAMPLE-brn1-60s-20ep.wav`)\n",
    ">\n",
    "><audio width=\"320\" height=\"160\"\n",
    ">\t\tsrc=\"./resources/audio/finetuned-results/EXAMPLE-brn1-60s-20ep.wav\"\n",
    ">\t\tcontrols>\n",
    ">\n",
    ">- **Epochs: 40** (`EXAMPLE-brn1-60s-40ep.wav`)\n",
    ">\n",
    "><audio width=\"320\" height=\"160\"\n",
    ">\t\tsrc=\"./resources/audio/finetuned-results/EXAMPLE-brn1-60s-40ep.wav\"\n",
    ">\t\tcontrols>\n",
    ">\n",
    "\n",
    "Because of the small scale of these experiments, it's hard to make a very confident recommendation, but I'll try anyways. I expect some value between 10 and 25 epochs to give the best compromise between naturalness and resemblance to the target voice. Feel free to experiment outside of those ranges and prove me wrong!\n",
    "\n",
    "In terms of data amount, I believe 30-60 seconds may be enough.\n",
    "\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "Hopefully, this notebook has brought you some new insight into how fine-tuning a model works, perhaps led you to dig through some of the code that makes it happen, and also made you aware of some of the challenges that come with trying to fine-tune a model. Feel free to experiment some more!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-freevc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
