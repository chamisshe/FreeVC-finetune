{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning FreeVC\n",
    "\n",
    "\n",
    "Hi!\n",
    "\n",
    "This notebook takes you through the steps to **fine-tune** the **Voice Conversion** model **FreeVC**. It is meant to be beginner-friendly, sparing you (and myself, the author of this notebook) most of the details of FreeVC's incredibly complicated Architecture. To those looking for a deeper dive into some of the concepts, models and techniques on which FreeVC is built, there will be some links to further reading.\n",
    "\n",
    "> **FreeVC: [Paper](https://arxiv.org/abs/2210.15418) | [Demo](https://olawod.github.io/FreeVC-demo/) | [Code](https://github.com/OlaWod/FreeVC)**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove before shipping\n",
    "# ! python initialize.py --leave_chunks\n",
    "# DO_NOT_CHUNK = True\n",
    "# DO_NOT_CHUNK = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "### What is Voice Conversion?\n",
    "\n",
    "Voice conversion is probably best explained using an example: There are two people, Alice and Bob. Alice wants to impersonate Bob, and she has a recording of Bob's voice. Alice then makes recordings of herself, saying things Bob would never say, such as \"give lots of money to Alice\". Using the recording she has of Bob, she *converts* the voice to sounding like Bob is saying all these ridiculous things.\n",
    "\n",
    "In this scenario, Alice is the **source** and Bob is the **target voice**. The distinction may seem a bit arbitrary at first, since we need both a recording of Alice *and* Bob, but since Bob's voice is where we want to end up, this distinction makes sense.\n",
    "\n",
    "#### Intuition\n",
    "Without going into technical details, let's ask why this works *intuitively*.\n",
    "\n",
    "When we listen to a voice, our brains more or less automatically process two different things:\n",
    "- *Who is speaking?* (let's call that **identity** or **speaker information**)\n",
    "- *What is being said?* (let's call that **content**)\n",
    "Determining the identity comes down to factors both physically inherent to your voice - mainly timbre and pitch - as well as factors that are more under the speaker's direct control, things like accent, rhythm.\n",
    "\n",
    "The content is largely independent of the features that make up identity. Communication works because people with different voices are able to produce the same phonemes. The same sequence of vowels and consonants - be it a word, sentence or speech - means the same thing across different speakers.\n",
    "\n",
    "The identity and content information on a *signal* (a spoken utterance) seem to be independent of each other, to a certain degree. The core idea for voice conversion therefore is the following:\n",
    "1. Given a source signal (by speaker A), strip it all its speaker information, while preserving content. \n",
    "2. Extract target speaker information (speaker B).\n",
    "3. Insert target speaker information into the stripped source signal\n",
    "\n",
    "<details>\n",
    "\n",
    "**<summary>Difference to Voice Cloning</summary>**\n",
    "\n",
    "How does voice conversion differ from voice cloning?\n",
    "\n",
    "Fundamentally, voice cloning - the extracting of speaker features and using them to generate speech - falls in the realm of text-to-speech, whilst voice conversion is speech-to-speech and entirely textless. There are also some significant practical differences: For instance, sophisticated voice cloning models also preserve properties such as accent and rhythm, whereas voice conversion does not. The stripped signal in voice conversion is much more of a rigid template than the text input used in voice cloning. The advantage of this is that it allows the user more direct control over the rhythm, accent, pitch contour and so on, simply by having the desired patterns in the source signal.\n",
    "\n",
    "</details>\n",
    "<!-- such as timbre (the inherent sound of your voice - German has the beautiful word *Klangfarbe* for it, which literally translates to \"Sound colour\"), pitch, -->\n",
    "<!-- The goal of voice conversion is to make a recorded utterance by *speaker A* sound like it's being said by *speaker B*. To do so, we essentially want to remove the speaker-specific features of speaker A from the recording, and replace them with speaker B's features. This is different from similar technology such as voice cloning, which achieves a similar output, but is still text-to-speech. -->\n",
    "\n",
    "### FreeVC: Architecture\n",
    "\n",
    "A significant part of FreeVC's architecture is based on [**VITS**](https://github.com/jaywalnut310/vits), an **end-to-end text-to-speech** model. VITS is a popular model because its output sounds very human for a TTS system. A core piece of VITS is the [Conditional Variational Autoencoder](https://theaiacademy.blogspot.com/2020/05/understanding-conditional-variational.html), a type of autoencoder more suitable for d\n",
    " However, as a TTS system it uses text as the input, whereas voice conversion is a speech-to-speech task. Roughly summarized and glossing over many technical details, FreeVC modifies the architecture of VITS in two ways: \n",
    "\n",
    "Firstly, it replaces the text encoder with an encoder capable of handling speech. The encoder itself consists of multiple pieces: A pretrained model that transforms the raw waveform into a vector (WavLM), a *bottleneck extractor* that reduces the dimensionality of the obtained vector and hopefully sieves out the information not needed, and lastly a normalizing flow (read more on flow [here](https://towardsdatascience.com/introduction-to-normalizing-flows-d002af262a4b)).\n",
    "\n",
    "Secondly, it adds a pretrained **speaker encoder** to the model architecture. A mel-spectrogram of the audio sample of the target speaker is given as input to the speaker encoder. It extracts the relevant speaker features, and feeds the resulting *speaker embedding* into both the flow module and the decoder.\n",
    "\n",
    "Finally, the decoder takes the encoded source audio and the speaker embedding, and creates the output waveform from it. As in VITS, FreeVC also uses [HiFi-GAN V1](https://github.com/jik876/hifi-gan) as its decoder.\n",
    "\n",
    "Additionally, FreeVC uses a discriminator to incorporate [adversarial learning](https://developers.google.com/machine-learning/gan/) as well as data augmentation during training of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "\n",
    "\n",
    "### Installation/Prerequisites\n",
    "- Python 3.9.19\n",
    "- WavLM\n",
    "- HiFiGAN\n",
    "- venv:\n",
    "    - `pip install -r requirements.txt`\n",
    "    - TODO: ensure the following packages are included in the requirements\n",
    "        - protobuf<=3.20.3\n",
    "        - six==1.16.0\n",
    "        - matplotlib\n",
    "        - numpy<=1.22.4\n",
    "- ffmpeg: to enable exporting as flac\n",
    "    - `sudo apt update && sudo apt upgrade` `sudo apt install ffmpeg`\n",
    "    - `ffmpeg -version` to check the installation \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Venv & install requirements\n",
    "\n",
    "\n",
    "#### Ensure Prerequisites\n",
    "Firstly, make sure that Python 3.9 and FFmpeg are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.19\n",
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "libavutil      56. 70.100 / 56. 70.100\n",
      "libavcodec     58.134.100 / 58.134.100\n",
      "libavformat    58. 76.100 / 58. 76.100\n",
      "libavdevice    58. 13.100 / 58. 13.100\n",
      "libavfilter     7.110.100 /  7.110.100\n",
      "libswscale      5.  9.100 /  5.  9.100\n",
      "libswresample   3.  9.100 /  3.  9.100\n",
      "libpostproc    55.  9.100 / 55.  9.100\n"
     ]
    }
   ],
   "source": [
    "# Check for Python 3.9\n",
    "! python3.9 -V\n",
    "# Check for FFmpeg\n",
    "! ffmpeg -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "**<summary>Install Python 3.9 & FFmpeg</summary>**\n",
    "\n",
    "If checking with the above commands gives you an error, follow these steps:\n",
    "\n",
    "##### Python 3.9\n",
    "```bash\n",
    "sudo add-apt-repository ppa:deadsnakes/ppa\n",
    "sudo apt-get update && sudo apt upgrade\n",
    "sudo apt-get install python3.9\n",
    "```\n",
    "\n",
    "##### FFmpeg\n",
    "```bash\n",
    "sudo apt update\n",
    "sudo apt install ffmpeg\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Venv & Packages\n",
    "\n",
    "Next, as is standard procedure, we want to install the required modules inside a Virtual Environment (or venv). Because different projects have different dependencies, we want to keep them from interfering with each other, therefore we install the required dependencies in an isolated environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create venv\n",
    "! python3.9 -m venv .venv-freevc\n",
    "# activate venv\n",
    "! source .venv-freevc/bin/activate\n",
    "# install requirements\n",
    "! pip install -r requirements.txt\n",
    "# ! pip install ipykernel\n",
    "# add the venv to the registry of jupyter kernels, allowing us to use  allows jupyter \n",
    "! python3.9 -m ipykernel install --name=.venv-freevc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WavLM\n",
    "\n",
    "Download the `WavLM Large` model found on [this page](https://github.com/microsoft/unilm/tree/master/wavlm).\n",
    "> `Pre-Trained Models` > WavLM Large `Google Drive`\n",
    "\n",
    "Next, move the downloaded file into the `wavlm/` folder.\n",
    "\n",
    "You'll want to end up with the following folder structure:\n",
    "> ```ascii\n",
    "> FreeVC-finetune/\n",
    "> ├─ ...\n",
    "> ├─ wavlm/\n",
    "> │  ├─ modules.py/\n",
    "> │  ├─ WavLM-Large.pt\n",
    "> │  ├─ WavLM-Large.pt.txt\n",
    "> │  ├─ WavLM.py\n",
    "> │  ├─ __init__.py\n",
    "> ├─ ...\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HiFi-GAN\n",
    "Download the HiFi-GAN model `VCTK_V1` found on [this page](https://github.com/jik876/hifi-gan?tab=readme-ov-file). \n",
    "> `Download Pretrained Models` > Google Drive: `VCTK_V1` > Download `generator_v1`\n",
    "\n",
    "Next, move the downloaded file into the `hifigan/` folder.\n",
    "\n",
    "You'll want to end up with the following folder structure:\n",
    "> ```ascii\n",
    "> FreeVC-finetune/\n",
    "> ├─ ...\n",
    "> ├─ hifigan/\n",
    "> │  ├─ __init__.py\n",
    "> │  ├─ config.json\n",
    "> │  ├─ generator_v1\n",
    "> │  ├─ generator_v1.txt\n",
    "> │  ├─ models.py/\n",
    "> ├─ ...\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the user followed the previous two steps correctly\n",
    "import os\n",
    "file = \"wavlm/WavLM-Large.pt\"\n",
    "assert os.path.exists(file), f\"{file} is missing.\\nMake sure you downloaded the WavLM-Large model and put it in the correct directory\"\n",
    "file = \"hifigan/generator_v1\"\n",
    "assert os.path.exists(file), f\"{file} is missing.\\nMake sure you downloaded the HiFi-Gan model (VCTK_V1) and put it in the correct directory\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Preparation\n",
    "\n",
    "To finetune the pre-trained model, we of course need some training data to adapt the model to the target speaker. With _some_ training data, I mean a whole lot of it.\n",
    "\n",
    "We want to make sure we make the most of our data. Therefore, we'll do some simple preprocessing on it.\n",
    "\n",
    "#### Chop it up\n",
    "\n",
    "The **base model** (i.e. FreeVC's pretrained model that we're finetuning) is trained on the [VCTK corpus](https://datashare.ed.ac.uk/handle/10283/3443). The audio in this corpus is stored in multiple smaller files (3.4 seconds on average) than one large file.\n",
    "\n",
    "If your finetuning-data is already in similarly sized chunks, you can **skip this step**. Otherwise, run the following cells on your file(s): This will automatically detect silent passages - for example between sentences, and thus split your audio into adequately sized chunks.\n",
    "\n",
    "You can modify the following parameters to control the chunking:\n",
    "- `MIN_SILENCE_LEN` (miliseconds): Defines the minimal length of silence necessary to split the audio at that point\n",
    "- `SILENCE_THRESHOLD` (dBFS): Defines what counts as silent and what does not; anything louder than the set threshold will count as not silent. \n",
    "- `MIN_CHUNK_LEN` (seconds): Any chunk shorter than this value will be discarded and NOT saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified code, originally from:\n",
    "#   https://www.codespeedy.com/split-audio-files-using-silence-detection-in-python/\n",
    "#   retrieved on 2024-08-23\n",
    "import os\n",
    "import copy\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "from typing import Union\n",
    "\n",
    "def chunk_audio(filelist: list[str],silence_len=800,silence_thr=-40, chunklen: float=0., training_len: int=300, out_path=\"./chunks\", quiet: bool=False):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    ---\n",
    "        filelist: \n",
    "        silence_len:\n",
    "        silence_thr:\n",
    "        chunklen:\n",
    "        training_len: intended length of the training data (seconds)\n",
    "    \"\"\"\n",
    "    # necessary to avoid outside-scope filelist being emptied inside current function\n",
    "    filelist = copy.copy(filelist)\n",
    "    count = 0\n",
    "    length = 0.\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "    while length <= training_len and filelist != []:\n",
    "        print(\"in while loop atm\")\n",
    "        for file in filelist:\n",
    "            filelist.remove(file)\n",
    "            print(\"a\")\n",
    "            sound = AudioSegment.from_wav(file)\n",
    "            print(\"b\")\n",
    "            # print(sound.duration_seconds)\n",
    "            # spliting audio files\n",
    "            audio_chunks = split_on_silence(sound, min_silence_len=silence_len, silence_thresh=silence_thr)\n",
    "            print(\"c\")\n",
    "            #loop is used to iterate over the output list\n",
    "            for pre_chunk in audio_chunks:\n",
    "                # save them as a FLAC file\n",
    "                cut_chunks = _cut_chunks(pre_chunk)\n",
    "                for chunk in cut_chunks:\n",
    "                    if chunk.duration_seconds >= chunklen:\n",
    "                        output_file = \"{0}/chunk{1}.flac\".format(out_path, count+1)\n",
    "                        # if the current chunk will exceed the intended length of the training data,\n",
    "                        # cut it in order to exactly reach the training length\n",
    "                        if length+chunk.duration_seconds >= training_len:\n",
    "                            overlength = (length+chunk.duration_seconds)-training_len\n",
    "                            overlength_ms = round(overlength*1000)\n",
    "                            chunk = chunk[:-overlength_ms]\n",
    "                        if chunk.duration_seconds == 0: continue\n",
    "                        length += chunk.duration_seconds\n",
    "                        count += 1\n",
    "                        print(length)\n",
    "                        chunk.export(output_file, format=\"flac\")\n",
    "                        # skip printing if quiet-flag is set (exists mostly for not cluttering the testing)\n",
    "                        if quiet: continue\n",
    "                        print(\"Exported file\", output_file, \"({0})\".format(len(chunk)))\n",
    "                    else:\n",
    "                        if quiet: continue\n",
    "                        print(\"Skipping Chunk: Too short (< {0} seconds)\".format(chunklen))\n",
    "    if not quiet:\n",
    "        print(\"\\nAverage length of saved chunks: {0} Seconds\".format(round(length/count,2)))\n",
    "        print(\"\\nTotal length of saved chunks: {0} Seconds\".format(round(length,2)))\n",
    "\n",
    "\n",
    "def _cut_chunks(chunk: AudioSegment):\n",
    "    out_list = []\n",
    "    if chunk.duration_seconds >= 4:\n",
    "        total_len = len(chunk)\n",
    "        half_len = total_len // 2\n",
    "        new_chunks = [chunk[:half_len], chunk[half_len:]]\n",
    "        for c in new_chunks:\n",
    "            out_list.extend(_cut_chunks(c))\n",
    "        return out_list\n",
    "    else:\n",
    "        return [chunk]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in while loop atm\n",
      "a\n",
      "b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/notmyyka/UZH/pp_vc/FreeVC/.venv-freevc/lib/python3.9/site-packages/pydub/audio_segment.py:808: ResourceWarning: unclosed file <_io.BufferedReader name='./test/resources/test_audio_12s.wav'>\n",
      "  return cls.from_file(file, 'wav', parameters=parameters)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n",
      "1.9640136054421768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1748/3264900338.py:52: ResourceWarning: unclosed file <_io.BufferedRandom name='./test/resources/temp/chunk1.flac'>\n",
      "  chunk.export(output_file, format=\"flac\")\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/tmp/ipykernel_1748/3264900338.py:52: ResourceWarning: unclosed file <_io.BufferedRandom name='./test/resources/temp/chunk2.flac'>\n",
      "  chunk.export(output_file, format=\"flac\")\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "test_number_of_files (__main__.TestChunkAudio) ... ok\n",
      "test_total_length (__main__.TestChunkAudio) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 2.814s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "# Unittest of function chunk_audio(), with setup/teardown fixtures\n",
    "#   Testing is limited to a single set of parameters.\n",
    "import unittest\n",
    "import os\n",
    "import shutil\n",
    "from pydub import AudioSegment\n",
    "\n",
    "class TestChunkAudio(unittest.TestCase):\n",
    "    pass\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        cls.out_path = \"./test/resources/temp\"\n",
    "        os.mkdir(cls.out_path)\n",
    "        chunk_audio(filelist=[\"./test/resources/test_audio_12s.wav\"], silence_len=800, silence_thr=-40, chunklen=1.5, training_len=6, out_path=cls.out_path, quiet=True)\n",
    "\n",
    "    def test_number_of_files(self):\n",
    "        # check number of generated files against expected number\n",
    "        self.assertEqual(len(os.listdir(self.out_path)), 2)\n",
    "\n",
    "    def test_total_length(self):\n",
    "        # sum length of chunks...\n",
    "        total_len = sum([AudioSegment.from_file(f'{self.out_path}/{file}', format=\"flac\").duration_seconds for file in os.listdir(self.out_path)])\n",
    "        # ...check against expected length\n",
    "        print(total_len)\n",
    "        self.assertEqual(total_len,6.0)\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        shutil.rmtree(cls.out_path)\n",
    "        # for file in os.listdir(cls.out_path):\n",
    "\n",
    "res = unittest.main(argv=[''], verbosity=3, exit=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Chunk: Too short (< 1.5 seconds)\n",
      "Skipping Chunk: Too short (< 1.5 seconds)\n",
      "Exported file ./z_testchunks/chunk1.flac (1964)\n",
      "Exported file ./z_testchunks/chunk2.flac (4036)\n",
      "Skipping Chunk: Too short (< 1.5 seconds)\n",
      "Skipping Chunk: Too short (< 1.5 seconds)\n",
      "Skipping Chunk: Too short (< 1.5 seconds)\n",
      "Skipping Chunk: Too short (< 1.5 seconds)\n",
      "Skipping Chunk: Too short (< 1.5 seconds)\n",
      "Skipping Chunk: Too short (< 1.5 seconds)\n",
      "Skipping Chunk: Too short (< 1.5 seconds)\n",
      "Skipping Chunk: Too short (< 1.5 seconds)\n",
      "Skipping Chunk: Too short (< 1.5 seconds)\n",
      "Skipping Chunk: Too short (< 1.5 seconds)\n",
      "Skipping Chunk: Too short (< 1.5 seconds)\n",
      "Skipping Chunk: Too short (< 1.5 seconds)\n",
      "Skipping Chunk: Too short (< 1.5 seconds)\n",
      "Skipping Chunk: Too short (< 1.5 seconds)\n",
      "Skipping Chunk: Too short (< 1.5 seconds)\n",
      "\n",
      "Average length of saved chunks: 3.0 Seconds\n",
      "\n",
      "Total length of saved chunks: 6.0 Seconds\n"
     ]
    }
   ],
   "source": [
    "# VARIABLES\n",
    "MIN_SILENCE_LEN = 800\n",
    "SILENCE_THRESHOLD = -40\n",
    "MIN_CHUNK_LEN = 1.5\n",
    "\n",
    "# USER TODO: list of files to chunk\n",
    "filelist = [\"./bernie_filibuster_22sec.wav\"]\n",
    "filelist = [\"./LN_AUDIOFILES/brn1/bernie_filibuster_pt1_5min.wav\"]\n",
    "\n",
    "# TODO remove before shipping\n",
    "# if not DO_NOT_CHUNK:\n",
    "chunk_audio(filelist, silence_len=MIN_SILENCE_LEN, silence_thr=SILENCE_THRESHOLD, chunklen=MIN_CHUNK_LEN, training_len=6, out_path=\"./z_testchunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Chunk: Too short (< 1.5 seconds)\n",
      "Skipping Chunk: Too short (< 1.5 seconds)\n",
      "Exporting file ./test/resources/temp/chunk1.flac (1964)\n",
      "Exporting file ./test/resources/temp/chunk2.flac (3036)\n",
      "\n",
      "Average length of saved chunks: 2.5 Seconds\n",
      "\n",
      "Total length of saved chunks: 5.0 Seconds\n"
     ]
    }
   ],
   "source": [
    "# from pydub import AudioSegment\n",
    "\n",
    "# audio = \"bernie_filibuster_22sec.wav\"\n",
    "\n",
    "# sound = AudioSegment.from_file(audio, format=\"wav\")\n",
    "\n",
    "# outpath = \"./test/resources/test_audio_12s.wav\"\n",
    "\n",
    "# sound_10s = sound[:12000]\n",
    "\n",
    "# file_handle=sound_10s.export(outpath, format=\"wav\")\n",
    "chunk_audio(filelist=[\"./test/resources/test_audio_12s.wav\"], silence_len=800, silence_thr=-40, chunklen=1.5, training_len=5, out_path=\"./test/resources/temp\")\n",
    "\n",
    "for file in os.listdir(\"./test/resources/temp\"):\n",
    "    os.remove(\"./test/resources/temp/\"+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES\n",
    "MIN_SILENCE_LEN = 800\n",
    "SILENCE_THRESHOLD = -40\n",
    "MIN_CHUNK_LEN = 1.5\n",
    "\n",
    "# USER TODO: list of files to chunk\n",
    "filelist = [\"./LN_AUDIOFILES/brn1/bernie_filibuster_pt1_5min.wav\"]\n",
    "\n",
    "# TODO remove before shipping\n",
    "if not DO_NOT_CHUNK:\n",
    "    chunk_audio(filelist, silence_len=MIN_SILENCE_LEN, silence_thr=SILENCE_THRESHOLD, chunklen=MIN_CHUNK_LEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "At this point, we have some audio data in appropriately sized chunks. We now need to run some very particular preprocessing steps on it, so that the model receives it in the right format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storage Format\n",
    "\n",
    "FreeVC expects our fine-tuning data to be in the same format as its original training data - the [VCTK-dataset](https://datashare.ed.ac.uk/handle/10283/3443) . Therefore, we need to rename some files and move them to the right places before we run any preprocessing.\n",
    "\n",
    "You'll need to assign some **4-character** ID to your speaker - pick one that makes sense to you. If it's longer or shorter than 4 characters, this won't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: pick your Speaker ID\n",
    "SPEAKER_ID = \"brn1\"\n",
    "assert len(SPEAKER_ID) == 4\n",
    "\n",
    "# DIRECTORY NAMES\n",
    "CHUNKS = \"./chunks/\"\n",
    "FLACS = \"./dataset/flac/\"\n",
    "DATA_PATH = f'{FLACS}{SPEAKER_ID}/'\n",
    "DATA16K = \"dataset/finetuning-16k\"\n",
    "DATA22K = \"dataset/finetuning-22k\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will rename your audio and move it into a directory with the right structure.\n",
    "\n",
    "`<some_dir>/<sp_id>/<sp_id-filename>_mic2.flac`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    os.makedirs(DATA_PATH)\n",
    "\n",
    "for i,file in enumerate(os.listdir(CHUNKS)):\n",
    "    # print(file)\n",
    "    # rename & move files to the specific format necessary\n",
    "    new_filename = f'{SPEAKER_ID}-{i}_mic2.flac'\n",
    "    a =os.path.join(CHUNKS,file)\n",
    "    shutil.copy(a, os.path.join(DATA_PATH,new_filename))\n",
    "print(\"Moved and renamed your training files.\\nGreat Success!! Very Nice!\")\n",
    "# os.listdir(DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downsampling\n",
    "\n",
    "Downsamples the audio to 16kHz.\n",
    "- `--sr1` sampling rate`\n",
    "- `--sr2` sampling rate`\n",
    "- `--in_dir` path to source dir`\n",
    "- `--out_dir1` path to target dir`\n",
    "- `--out_dir2` path to target dir`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python downsample.py --in_dir $FLACS --out_dir1 $DATA16K --out_dir2 $DATA22K\n",
    "! ln -s $DATA16K DUMMY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Splitting\n",
    "\n",
    "Next, our fine-tuning data will need to be split into a training, test and validation set.\n",
    "\n",
    "The original splitting-script of FreeVC uses 2 chunks from each speaker for validation, 10 chunks for testing and the rest for training. With an average of around 400 chunks per speaker, this is an average test-split of 2.5%, and validation-split of 0.5%. To me, this seems like an overly small test and validation portion.\n",
    "\n",
    "Therefore the preprocessing script was modified:\n",
    "Before, the test and validation portions were constant, at 10 and 2 samples respectively. I changed them to a relative 5% and 1% portion for the test and validation sets.\n",
    "\n",
    "_(As to whether this improves or worsens performance, I have no empirical evidence for either and I do not intend to gather it.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_file=\"./filelists/finetune-val.txt\"\n",
    "test_file=\"./filelists/finetune-test.txt\"\n",
    "train_file=\"./filelists/finetune-train.txt\"\n",
    "# sr_wavs = f\"./dataset/sr/wav\"\n",
    "# ! python preprocess_flist.py --train_list $train_file --test_list $test_file --val_list $val_file --source_dir $sr_wavs\n",
    "! python preprocess_flist.py --train_list $train_file --test_list $test_file --val_list $val_file --source_dir $DATA16K\n",
    "! rm DUMMY\n",
    "! ln -s $DATA16K DUMMY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash -s \"$train_file\" \"$test_file\" \"$val_file\" \"$DATA16K\"\n",
    "# echo $1\n",
    "# python preprocess_flist.py --train_list $1 --test_list $2 --val_list $3 --source_dir $4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speaker Encoder (pretrained)\n",
    "\n",
    "Something something encode speaker information using a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare variable\n",
    "DATA_ROOT=\"./dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "! CUDA_VISIBLE_DEVICES=0 python preprocess_spk.py --in_dir $DATA16K --out_dir_root $DATA_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Augmentation\n",
    "\n",
    "To make the most of our data...    ...Spectrogram Resize (SR)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare variable\n",
    "HIFIGAN_CFG = \"hifigan/config.json\"\n",
    "WAV_DIR = \"dataset/sr/wav\"\n",
    "SSL_DIR = \"dataset/sr/wavlm\"\n",
    "\n",
    "# Perform data augmentation (spectrogram resize)\n",
    "! CUDA_VISIBLE_DEVICES=0 python preprocess_sr.py --in_dir $DATA22K --wav_dir $WAV_DIR --ssl_dir $SSL_DIR --config $HIFIGAN_CFG --min 68 --max 92 --sr 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning\n",
    "\n",
    "The hyperparameters of training are set in a JSON file, located in the `/configs/` directory. For finetuning, we'll use the file `freevc-finetune.json`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train freevc: use config 'configs/freevc-finetune.json', use model 'freevc'\n",
    "# ! CUDA_VISIBLE_DEVICES=0 python finetune.py --config configs/freevc-finetune.json --model freevc-finetune\n",
    "MODEL_NAME = f'freevc_finetune-{SPEAKER_ID}'\n",
    "MODEL_NAME = f'freevc_finetune'\n",
    "! echo $MODEL_NAME\n",
    "! CUDA_VISIBLE_DEVICES=0 python finetune.py -c configs/freevc-finetune.json -m $MODEL_NAME -d ./checkpoints/D-freevc.pth -g ./checkpoints/freevc.pth --force_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Output\n",
    "\n",
    "We're almost there.\n",
    "\n",
    "As a last step, we'll need to define the audio recording(s) that we actually want to convert to our target speakers. To do so, all we need to do is edit the file `convert.txt` - let's call it the **task file**. \n",
    "\n",
    "The structure of the task file is simple: \n",
    "- Each row corresponds to a single *task*, i.e. one source audio being converted to a target speaker.\n",
    "- There are three columns for each row, separated by a single pipe symbol (`|`). The first column defines the name of the task, the second column contains the path to the source file, and the third column contains the path to an audio file of the target speaker.\n",
    "\n",
    "This could look as follows: (Alice is the source, Bob the target speaker.)\n",
    "\n",
    "```txt\n",
    "alice2bob_1|PATH/TO/ALICE_1.wav|PATH/TO/BOB.wav\n",
    "alice2bob_2|PATH/TO/ALICE_2.wav|PATH/TO/BOB.wav\n",
    "alice2bob_3|PATH/TO/ALICE_3.wav|PATH/TO/BOB.wav\n",
    "...\n",
    "```\n",
    "\n",
    "- `alice2bob_X` is simply the name of the conversion - this will mainly be used to name the output file and in the logs.\n",
    "- `PATH/TO/ALICE_X.wav` is the path to the source files - recordings of Alice, which we want to convert to Bob's voice\n",
    "- `PATH/TO/BOB.wav` is the path to the audiofile of Bob's voice - the target. Note that this can be the same for various different source files.\n",
    "\n",
    "> **Tip &#x1F4A1;**\n",
    ">\n",
    "> Within the task file, we do **not** need to adhere to specific filenaming (such as 4-symbol speaker ids) \n",
    "\n",
    "Each user's task file will look different. Thus, this is something you'll have to do yourself. To help you however, there's a simple function to potentially make things a bit easier and faster. Note that it requires you to have all your source files in the same directory, and the name will be kept relatively simple. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_task_file(basename: str, source: str, target: str, taskfile: str=\"convert.txt\") -> str:\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    ---\n",
    "        basename: Name to be used as the base for naming each conversion. The name of the nth task will be 'basename_n'.\\\\\n",
    "        source: path to the DIRECTORY containing the source files.\\\\\n",
    "        target: path to the FILE containing the target speaker.\\\\\n",
    "        taskfile: allows the user to name their task file something other than 'convert.txt'.\\\\\n",
    "    \n",
    "    Output:\n",
    "    ---\n",
    "        taskfile\n",
    "    \"\"\"\n",
    "    # clear file\n",
    "    with open(taskfile, \"w\", encoding=\"utf-8\") as f: f.write()\n",
    "    # fill file\n",
    "    with open(taskfile, \"a\", encoding=\"utf-8\") as f:\n",
    "        for i,file in enumerate(os.listdir(source)):\n",
    "            f.write(f\"{basename}_{i}|{file}|{target}\")\n",
    "    return taskfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **&#x270F;&#xFE0F; To-do: User Entry**\n",
    ">\n",
    "> Enter the desired conversions into `convert.txt`.\n",
    "> You can use the function `fill_task_file()` to do this in a quick and simple way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER TODO: adjust parameters\n",
    "converttxt = fill_task_file(\"alice2bob\", source=\"PATH/TO/DIR/ALICE\", target=\"PATH/TO/FILE/BOB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! CUDA_VISIBLE_DEVICES=0 python convert.py --hpfile configs/freevc-finetune.json --ptfile checkpoints/freevc-finetune.pth --txtpath convert.txt --outdir outputs/freevc-finetune\n",
    "# ! CUDA_VISIBLE_DEVICES=0 python convert.py --hpfile configs/freevc.json --ptfile checkpoints/freevc.pth --txtpath convert.txt --outdir outputs/freevc-base\n",
    "! CUDA_VISIBLE_DEVICES=0 python convert.py --hpfile configs/freevc-finetune.json --ptfile logs/freevc-finetune/G_40.pth --txtpath convert.txt --outdir outputs/freevc-finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "! py finetune.py -c configs/freevc-finetune.json -m freevc-finetune -d ./checkpoints/D-freevc.pth -g ./checkpoints/freevc.pth --force_new\n",
    "! py convert.py --hpfile configs/freevc-finetune.json --ptfile logs/freevc-finetune/G_40.pth --txtpath convert.txt --outdir outputs/freevc-finetune\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Delete??\n",
    "# function that takes training audio and slices it to the desired length\n",
    "import pydub\n",
    "def make_audio_slices(audio_len: int, path: str):\n",
    "    # pydub works with miliseconds, not seconds\n",
    "    audio_len_ms = audio_len*1000\n",
    "\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "    \n",
    "def move_all_files(src, dst):\n",
    "    \"\"\"\n",
    "    move all files within the directory src to dst\n",
    "    \"\"\"\n",
    "    files = os.listdir(src)\n",
    "    if not os.path.exists(dst): \n",
    "        os.mkdir(dst)\n",
    "    for f in files:\n",
    "        src_path = os.path.join(src, f)\n",
    "        dst_path = os.path.join(dst, f)\n",
    "        shutil.move(src_path, dst_path)\n",
    "    \n",
    "\n",
    "def get_max_checkpoint(modelname: str):\n",
    "    \"\"\"\n",
    "    get the latest checkpoint for models where `eval_interval`<`epochs`, and thus intermediate checkpoints are stored as well. \n",
    "    \"\"\"\n",
    "    gen_checkpoints = [file for file in os.listdir(f\"./logs/{modelname}\") if file.startswith(\"G\") and file.endswith(\".pth\")]\n",
    "    max_pt = max([int(file.removeprefix(\"G_\").removesuffix(\".pth\")) for file in gen_checkpoints])\n",
    "    checkpoint = f'./logs/{modelname}/G_{max_pt}.pth'\n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# function used to wrap a single experiment, wraps several audio processing functions, training and conversion\n",
    "def train_and_generate(audio_len: int=300, epochs: int=5, training_data: str=\"\", keep_data: bool=True, new_config_pars: dict={}, speaker_id: str=\"\", force_train: bool=False):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    ---\n",
    "        **audio_len**: length of audio used for training in seconds. If the value exceeds the maximum possible value of the training data, it defaults to the maximum possible value\\\\\n",
    "        epochs: number of epochs used for finetuning\\\\\n",
    "        training_data: location of training data.\n",
    "        keep_data: flag that determines if the training data is kept or deleted. Includes everything from WAVs, FLACs, mel-spectrograms to speaker embeddings\\\\\n",
    "    Returns\n",
    "    ---\n",
    "\n",
    "    \"\"\"\n",
    "    # check that the following variables are indeed declared, by asserting that they aren't the default values.\n",
    "    #   You may ask, \"why have default values then, if you don't actually want those values?\".\n",
    "    #   Well, because I want to have them in that position and I have some default values declared before, \n",
    "    #   and python doesn't let me have any parameters without default values later.\n",
    "    #   Is it \"nice\" programming style? Probably not. Do I care? Not enough. Does it matter? Not really. Is this comment getting way too long? Yes.\n",
    "    assert training_data != \"\"\n",
    "    assert speaker_id != \"\"\n",
    "    assert new_config_pars != {}\n",
    "\n",
    "\n",
    "    # make changes to a copy of the finetune-config, leaving the original untouched\n",
    "    config_file = shutil.copy(\"./configs/freevc-finetune.json\", \"./configs/freevc-finetune-exp.json\")\n",
    "\n",
    "    if not os.path.exists(config_file): pass\n",
    "    \n",
    "    MODEL_NAME = f\"freevc-ft-{speaker_id}-{audio_len}s-{epochs}ep\"\n",
    "    \n",
    "        \n",
    "    if not os.path.exists(f'./logs/{MODEL_NAME}') or force_train:\n",
    "        with open(config_file, \"r\", encoding=\"utf-8\") as cfg:\n",
    "            config = json.load(cfg)\n",
    "        # modify config file (set epochs and filelists)\n",
    "        config[\"train\"][\"epochs\"] = epochs\n",
    "        config[\"train\"][\"eval_interval\"] = epochs\n",
    "        config[\"train\"][\"log_interval\"] = epochs\n",
    "        config[\"data\"][\"training_files\"] = new_config_pars[\"training_files\"]\n",
    "        config[\"data\"][\"validation_files\"] = new_config_pars[\"validation_files\"]\n",
    "        \n",
    "        with open(config_file, \"w\", encoding=\"utf-8\") as cfg:\n",
    "            json.dump(config, cfg, indent=4)\n",
    "\n",
    "        # MODEL_NAME = f'freevc_finetune-{SPEAKER_ID}'\n",
    "        ! CUDA_VISIBLE_DEVICES=0 python finetune.py -c configs/freevc-finetune-exp.json -m $MODEL_NAME -d ./checkpoints/D-freevc.pth -g ./checkpoints/freevc.pth --force_new\n",
    "\n",
    "    checkpoint = get_max_checkpoint(modelname=MODEL_NAME)\n",
    "    # TODO: convert.txt\n",
    "    converttxt = \"convert.txt\"\n",
    "    ! CUDA_VISIBLE_DEVICES=0 python convert.py --hpfile configs/freevc-finetune-exp.json --ptfile $checkpoint --txtpath $converttxt --outdir outputs/$MODEL_NAME\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "def wrap_experiments(training_amounts: list[int], different_epochs: list[int], training_data: list[str], speaker_id: str, keep_stored: bool=True, force_train: bool=True):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    ---\n",
    "        training_amounts: list of different amounts of training data (i.e. length of the audio training material, (in seconds))\\\\\n",
    "        different_epochs: list of different amounts of epochs to finetune the model on\\\\\n",
    "        training_data: list of paths of all files to be considered for training\\\\\n",
    "        speaker_id: 4-symbol ID of the speaker\\\\ \n",
    "        keep_stored: for different amounts of training data, instead of deleting the data simply \"archive it\" so it does not have to be regenerated in future experiments with the same amount.\n",
    "        force_train: if a finetuning configuration already exists, don't train, only convert\\\\\n",
    "    \"\"\"\n",
    "    # speaker_id is needed to name/identify training data \n",
    "    assert speaker_id != \"\"\n",
    "    assert os.listdir(\"./\")\n",
    "    \n",
    "    if not os.path.exists(\"./dataset/\"):\n",
    "        os.mkdir(\"./dataset\")\n",
    "    # idea: iterate through audio lengths first (all of which share the same training data).\n",
    "    #   allows us to keep the training data whilst experimenting with different number of epochs\n",
    "    for tl in training_amounts:\n",
    "        try:\n",
    "            # consistent naming scheme including speaker & amount of training data, used to keep training files stored (in ./depot/...)\n",
    "            depot_path = f\"./depot/{speaker_id}/{str(tl)}_sec\"\n",
    "\n",
    "            # adjust paths ONLY IF NECESSARY\n",
    "            DATA16K =  \"./dataset/finetuning-16k\"\n",
    "            CHUNKS =  \"./chunks/\"\n",
    "            DATA_ROOT =  \"./dataset\"\n",
    "            val_file=\"./filelists/finetune-val.txt\"\n",
    "            test_file =  \"./filelists/finetune-test.txt\"\n",
    "            train_file =  \"./filelists/finetune-train.txt\"\n",
    "            for dir in [DATA16K, CHUNKS, \"./filelists\"]:\n",
    "                if not os.path.exists(dir):\n",
    "                    os.mkdir(dir)\n",
    "\n",
    "            if os.path.exists(depot_path) and os.listdir(depot_path):\n",
    "                print(\"path exists, moving files from depot to active directory...\")\n",
    "                # \"recover\" existing data from the depot, avoid recreating data that already exists\n",
    "                #  data (./dataset)\n",
    "                move_all_files(src=f'{depot_path}/dataset', dst=\"./dataset\")\n",
    "                #  chunks\n",
    "                move_all_files(src=f'{depot_path}/chunks', dst=CHUNKS)\n",
    "                #  filelists\n",
    "                move_all_files(src=f'{depot_path}/filelists', dst=\"./filelists\")\n",
    "                #  DUMMY\n",
    "                ! ln -s $DATA16K DUMMY\n",
    "\n",
    "            # OR call audio pre-processing for complete training amount\n",
    "            else:\n",
    "                print(f'\\nPROCESSING AUDIO:\\n\\t Amount of Training Data: {tl}s')\n",
    "                # create chunks\n",
    "                print(training_data, tl)\n",
    "                chunk_audio(filelist=training_data, training_len=tl)\n",
    "\n",
    "                # declare variables necessary for DOWNSAMPLING\n",
    "                FLACS =  \"./dataset/flac/\"\n",
    "                DATA_PATH =  f'{FLACS}{speaker_id}/'\n",
    "                DATA22K =  \"./dataset/finetuning-22k\"\n",
    "                # create directories where necessary\n",
    "                for dir in [FLACS, DATA_PATH, DATA22K]:\n",
    "                    if not os.path.exists(dir):\n",
    "                        os.mkdir(dir)\n",
    "                if not os.path.exists(DATA_PATH):\n",
    "                    os.makedirs(DATA_PATH)\n",
    "                for i,file in enumerate(os.listdir(CHUNKS)):\n",
    "                    # print(file)\n",
    "                    # rename & move files to the specific format necessary\n",
    "                    new_filename = f'{SPEAKER_ID}-{i}_mic2.flac'\n",
    "                    a =os.path.join(CHUNKS,file)\n",
    "                    shutil.copy(a, os.path.join(DATA_PATH,new_filename))\n",
    "                # downsampling operation\n",
    "                print(\"Moved and renamed your training files.\\nGreat Success!! Very Nice!\")\n",
    "                ! python downsample.py --in_dir $FLACS --out_dir1 $DATA16K --out_dir2 $DATA22K\n",
    "\n",
    "\n",
    "\n",
    "                # declare variables necessary for CREATING THE FILELISTS\n",
    "                val_file= \"./filelists/finetune-val.txt\"\n",
    "                test_file =  \"./filelists/finetune-test.txt\"\n",
    "                train_file =  \"./filelists/finetune-train.txt\"\n",
    "                # create filelists\n",
    "                ! python preprocess_flist.py --train_list $train_file --test_list $test_file --val_list $val_file --source_dir $DATA16K\n",
    "                ! ln -s $DATA16K DUMMY\n",
    "\n",
    "                # declare variables necessary for SPEAKER ENCODING\n",
    "                ! CUDA_VISIBLE_DEVICES=0 python preprocess_spk.py --in_dir $DATA16K --out_dir_root $DATA_ROOT\n",
    "\n",
    "                # declare variables necessary for DATA AUGMENTATION\n",
    "                HIFIGAN_CFG =  \"hifigan/config.json\"\n",
    "                WAV_DIR =  \"dataset/sr/wav\"\n",
    "                SSL_DIR =  \"dataset/sr/wavlm\"\n",
    "                # Perform data augmentation (spectrogram resize)\n",
    "                ! CUDA_VISIBLE_DEVICES=0 python preprocess_sr.py --in_dir $DATA22K --wav_dir $WAV_DIR --ssl_dir $SSL_DIR --config $HIFIGAN_CFG --min 68 --max 92 --sr 16000\n",
    "\n",
    "            # make sure filelists actually exist\n",
    "            for file in [val_file, train_file, test_file]:\n",
    "                assert os.path.exists(file)\n",
    "\n",
    "            # ### EPOCH LOOP ####\n",
    "            # finetune the model with different numbers of epochs, but using the same amount of training data \n",
    "            for eps in different_epochs:\n",
    "                # adjust config\n",
    "                add_to_config= {\"training_files\": train_file, \"validation_files\": val_file}\n",
    "                \n",
    "                # call training functions\n",
    "                train_and_generate(audio_len=tl, epochs=eps, training_data=\"?\", keep_data=True, new_config_pars=add_to_config, speaker_id=speaker_id, force_train=force_train)\n",
    "            # ### #### #### ####\n",
    "            \n",
    "        finally:\n",
    "            if keep_stored:\n",
    "                # keep files and simply move them to the depot\n",
    "                if not os.path.exists(depot_path):\n",
    "                    os.makedirs(depot_path)\n",
    "\n",
    "                move_all_files(src=\"./dataset\", dst=f'{depot_path}/dataset')\n",
    "                #  chunks\n",
    "                move_all_files(src=\"./chunks\", dst=f'{depot_path}/chunks')\n",
    "                #  filelists\n",
    "                move_all_files(src=\"./filelists\", dst=f'{depot_path}/filelists')\n",
    "                #  DUMMY\n",
    "                ! rm DUMMY\n",
    "                pass\n",
    "            else:\n",
    "                # DELETE existing chunks (./chunks) and data (./dataset)for the current training amount\n",
    "                #  data (./dataset)\n",
    "                shutil.rmtree(\"./dataset\")\n",
    "                #  chunks\n",
    "                shutil.rmtree(CHUNKS)\n",
    "                #  filelists\n",
    "                for file in [val_file, train_file, test_file]:\n",
    "                    os.remove(file)\n",
    "                #  DUMMY\n",
    "                ! rm DUMMY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/notmyyka/UZH/pp_vc/FreeVC/preprocess_flist.py\", line 28, in <module>\n",
      "    assert total_len>=10, \"message something\"\n",
      "AssertionError: message something\n"
     ]
    }
   ],
   "source": [
    "val_file= \"./filelists/finetune-val.txt\"\n",
    "test_file =  \"./filelists/finetune-test.txt\"\n",
    "train_file =  \"./filelists/finetune-train.txt\"\n",
    "src_dir = \"./depot/brn1/4_sec/dataset/finetuning-16k/\"\n",
    "# create filelists\n",
    "! python preprocess_flist.py --train_list $train_file --test_list $test_file --val_list $val_file --source_dir $src_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROCESSING AUDIO:\n",
      "\t Amount of Training Data: 4s\n",
      "['bernie_filibuster_22sec.wav'] 4\n",
      "in while loop atm\n",
      "a\n",
      "b\n",
      "c\n",
      "1.4449886621315193\n",
      "Exported file ./chunks/chunk1.flac (1445)\n",
      "2.213968253968254\n",
      "Exported file ./chunks/chunk2.flac (769)\n",
      "3.999954648526077\n",
      "Exported file ./chunks/chunk3.flac (1786)\n",
      "\n",
      "Average length of saved chunks: 1.33 Seconds\n",
      "\n",
      "Total length of saved chunks: 4.0 Seconds\n",
      "Moved and renamed your training files.\n",
      "Great Success!! Very Nice!\n",
      "0it [00:00, ?it/s]brn1\n",
      "./dataset/flac/brn1/brn1-0_mic2.flac\n",
      "brn1\n",
      "processing  ./dataset/flac/brn1/brn1-0_mic2.flac\n",
      "./dataset/flac/brn1/brn1-1_mic2.flac\n",
      "processing  brn1./dataset/flac/brn1/brn1-1_mic2.flac\n",
      "\n",
      "./dataset/flac/brn1/brn1-2_mic2.flac\n",
      "processing  ./dataset/flac/brn1/brn1-2_mic2.flac\n",
      "3it [00:00,  3.45it/s]\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 13842.59it/s]\n",
      "Writing ./filelists/finetune-train.txt\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 37117.73it/s]\n",
      "Writing ./filelists/finetune-val.txt\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 39945.75it/s]\n",
      "Writing ./filelists/finetune-test.txt\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 39945.75it/s]\n",
      "Number of workers:  12\n",
      "[INFO] spk_embed_out_dir:  ./dataset/spk\n",
      "Preprocessing brn1 ...\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]Loaded the voice encoder model on cuda in 5.54 seconds.\n",
      "Loaded the voice encoder model on cuda in 5.57 seconds.\n",
      "Loaded the voice encoder model on cuda in 5.57 seconds.\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:05<00:00,  1.99s/it]\n",
      "DONE!\n",
      "Loading WavLM for content...\n",
      "INFO:wavlm.WavLM:WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}\n",
      "Loaded WavLM.\n",
      "Loading vocoder...\n",
      "Removing weight norm...\n",
      "Loaded vocoder.\n",
      "['./dataset/finetuning-22k/brn1/brn1-1.wav', './dataset/finetuning-22k/brn1/brn1-2.wav', './dataset/finetuning-22k/brn1/brn1-0.wav']\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:15<00:00,  5.08s/it]\n",
      "INFO:freevc-ft-brn1-4s-4ep:{'train': {'log_interval': 4, 'eval_interval': 4, 'seed': 1234, 'epochs': 4, 'learning_rate': 0.0002, 'betas': [0.8, 0.99], 'eps': 1e-09, 'batch_size': 16, 'fp16_run': False, 'lr_decay': 0.999875, 'segment_size': 8960, 'init_lr_ratio': 1, 'warmup_epochs': 0, 'c_mel': 45, 'c_kl': 1.0, 'use_sr': True, 'max_speclen': 128, 'port': '8001'}, 'data': {'training_files': './filelists/finetune-train.txt', 'validation_files': './filelists/finetune-val.txt', 'max_wav_value': 32768.0, 'sampling_rate': 16000, 'filter_length': 1280, 'hop_length': 320, 'win_length': 1280, 'n_mel_channels': 80, 'mel_fmin': 0.0, 'mel_fmax': None}, 'model': {'inter_channels': 192, 'hidden_channels': 192, 'filter_channels': 768, 'n_heads': 2, 'n_layers': 6, 'kernel_size': 3, 'p_dropout': 0.1, 'resblock': '1', 'resblock_kernel_sizes': [3, 7, 11], 'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'upsample_rates': [10, 8, 2, 2], 'upsample_initial_channel': 512, 'upsample_kernel_sizes': [16, 16, 4, 4], 'n_layers_q': 3, 'use_spectral_norm': False, 'gin_channels': 256, 'ssl_dim': 1024, 'use_spk': True}, 'model_dir': './logs/freevc-ft-brn1-4s-4ep', 'generator': './checkpoints/freevc.pth', 'discriminator': './checkpoints/D-freevc.pth', 'force_new': True}\n",
      "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n",
      "train_loader:  1\n",
      "./checkpoints/freevc.pth\n",
      "INFO:freevc-ft-brn1-4s-4ep:Loaded checkpoint './checkpoints/freevc.pth' (iteration 1372)\n",
      "./checkpoints/D-freevc.pth\n",
      "INFO:freevc-ft-brn1-4s-4ep:Loaded checkpoint './checkpoints/D-freevc.pth' (iteration 1372)\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "INFO:root:Reducer buckets have been rebuilt in this iteration.\n",
      "INFO:freevc-ft-brn1-4s-4ep:Train Epoch: 1 [0%]\n",
      "INFO:freevc-ft-brn1-4s-4ep:[2.712888717651367, 2.825178623199463, 13.270971298217773, 23.9014835357666, 27.181232452392578, 0, 0.00016845718778664938]\n",
      "DEBUG:matplotlib:matplotlib data path: /home/notmyyka/UZH/pp_vc/FreeVC/.venv-freevc/lib/python3.9/site-packages/matplotlib/mpl-data\n",
      "DEBUG:matplotlib:CONFIGDIR=/home/notmyyka/.config/matplotlib\n",
      "DEBUG:matplotlib:interactive is False\n",
      "DEBUG:matplotlib:platform is linux\n",
      "filename:  DUMMY/brn1/brn1-2.wav\n",
      "INFO:freevc-ft-brn1-4s-4ep:Saving model and optimizer state at iteration 1 to ./logs/freevc-ft-brn1-4s-4ep/G_0.pth\n",
      "INFO:freevc-ft-brn1-4s-4ep:Saving model and optimizer state at iteration 1 to ./logs/freevc-ft-brn1-4s-4ep/D_0.pth\n",
      "INFO:freevc-ft-brn1-4s-4ep:====> Epoch: 1\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "INFO:root:Reducer buckets have been rebuilt in this iteration.\n",
      "INFO:freevc-ft-brn1-4s-4ep:====> Epoch: 2\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "INFO:freevc-ft-brn1-4s-4ep:====> Epoch: 3\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "INFO:freevc-ft-brn1-4s-4ep:====> Epoch: 4\n",
      "Loading model...\n",
      "Loading checkpoint...\n",
      "./logs/freevc-ft-brn1-4s-4ep/G_0.pth\n",
      "INFO:root:Loaded checkpoint './logs/freevc-ft-brn1-4s-4ep/G_0.pth' (iteration 1)\n",
      "Loading WavLM for content...\n",
      "INFO:wavlm.WavLM:WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}\n",
      "Loading speaker encoder...\n",
      "Loaded the voice encoder model on cuda in 0.02 seconds.\n",
      "Processing text...\n",
      "Synthesizing...\n",
      "2it [00:02,  1.38s/it]\n"
     ]
    }
   ],
   "source": [
    "# [6, 60] sec [2, 5] ep: TIME=\n",
    "# training_amts= [12,60,300]\n",
    "# epochs = [1,2,5,10]\n",
    "training_amts= [4]\n",
    "epochs = [4]\n",
    "# USER TODO\n",
    "SPEAKER_ID = \"brn1\"\n",
    "assert len(SPEAKER_ID) == 4\n",
    "\n",
    "training_data = [\"./LN_AUDIOFILES/brn1/bernie_filibuster_pt1_5min.wav\"]\n",
    "training_data = [\"./LN_AUDIOFILES/brn1/bernie_filibuster_pt1.wav\"]\n",
    "training_data = [\"bernie_filibuster_22sec.wav\"]\n",
    "wrap_experiments(training_amts, epochs, training_data=training_data, speaker_id=SPEAKER_ID, force_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "if os.path.exists(\"./filelists\") and not os.listdir(\"./filelists\"):\n",
    "    os.rmdir(\"./filelists\")\n",
    "\n",
    "if os.path.exists(\"./dataset\") and not os.listdir(\"./dataset\"):\n",
    "    os.rmdir(\"./dataset\")\n",
    "    # os.mkdir(\"./dataset\")\n",
    "\n",
    "os.path.exists(\"./dataset\")\n",
    "\n",
    "if os.path.exists(\"./depot/brn1/2_sec\"):\n",
    "    shutil.rmtree(\"./depot/brn1/2_sec\")\n",
    "if os.path.exists(\"./depot/brn1/4_sec\"):\n",
    "    shutil.rmtree(\"./depot/brn1/4_sec\")\n",
    "if os.path.exists(\"./depot/brn1/6_sec\"):\n",
    "    shutil.rmtree(\"./depot/brn1/6_sec\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_len = 4\n",
    "\n",
    "n_test = max(round(total_len*0.05), 1)\n",
    "        # num val set\n",
    "n_val = max(round(total_len*0.01), 1)\n",
    "# num train set\n",
    "n_train = total_len-(n_test+n_val)\n",
    "# just making sure\n",
    "assert total_len == n_test+n_val+n_train\n",
    "n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test directory moving\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def populate_dir(dir:str, num_dummyfiles:int):\n",
    "    for i in range(num_dummyfiles):\n",
    "        with open(f'{dir}/{i}.txt', \"w\") as f:\n",
    "            f.write(\"x\")\n",
    "\n",
    "dirname = \"justfortest\"\n",
    "os.mkdir(dirname)\n",
    "depot = f'{dirname}/depot'\n",
    "os.mkdir(depot)\n",
    "dataset_dir = f'{depot}/dataset'\n",
    "os.mkdir(dataset_dir)\n",
    "os.mkdir(f'{dataset_dir}/a')\n",
    "os.mkdir(f'{dataset_dir}/b')\n",
    "populate_dir(f'{dataset_dir}/a',2)\n",
    "populate_dir(f'{dataset_dir}/b',2)\n",
    "populate_dir(dataset_dir, 10)\n",
    "main = f'{dirname}/main'\n",
    "os.mkdir(main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'justfortest/main/dataset'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.move(dataset_dir, f'{main}/dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(dirname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = [\n",
    "    (),\n",
    "            ]\n",
    "\n",
    "\n",
    "for x,y in settings:\n",
    "    \n",
    "    ! CUDA_VISIBLE_DEVICES=0 python finetune.py -c configs/freevc-finetune.json -m freevc-finetune -d ./checkpoints/D-freevc.pth -g ./checkpoints/freevc.pth --force_new\n",
    "    ! CUDA_VISIBLE_DEVICES=0 python convert.py --hpfile configs/freevc-finetune.json --ptfile logs/freevc-finetune/G_40.pth --txtpath convert.txt --outdir outputs/freevc-finetune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import IPython\n",
    "ref_1 = \"\"\n",
    "print(\"Ref 1\")\n",
    "IPython.display.display(ipd.Audio(ref_1.numpy(), rate=sr))\n",
    "print(\"Example 1\")\n",
    "IPython.display.display(ipd.Audio(example_1.numpy(), rate=sr))\n",
    "print(\"Example 2\")\n",
    "IPython.display.display(ipd.Audio(example_2.numpy(), rate=sr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE LATER, JUST STATS\n",
    "\n",
    "import os\n",
    "def test_split(total_len):\n",
    "    n_test = max(round(total_len*0.05), 1)\n",
    "    n_val = max(round(total_len*0.01), 1)\n",
    "    n_train = total_len-(n_test+n_val)\n",
    "\n",
    "    assert total_len == n_test+n_val+n_train\n",
    "    print(total_len, \":\\t\",n_train,\", \",n_test,\", \", n_val)\n",
    "    assert total_len>=10, \"message something\"\n",
    "\n",
    "dir = os.path.abspath(\"~/\")\n",
    "os.walk(dir)\n",
    "vctk_path = os.path.abspath(\"../../../../../../mnt/c/Users/mhess/Downloads/VCTK-Corpus-0.92/wav48_silence_trimmed\")\n",
    "dirlist = os.listdir(vctk_path)\n",
    "# TODO: get avg number of chunks/speaker in vctk\n",
    "counter = 0\n",
    "num_chunks = 0\n",
    "for el in dirlist:\n",
    "    combined_path = os.path.join(vctk_path,el)\n",
    "    # print(f'{el}:\\t{os.path.isdir(combined_path)}')\n",
    "    if os.path.isdir(combined_path):\n",
    "        counter += 1\n",
    "        num_chunks += (len(os.listdir(combined_path)))/2\n",
    "\n",
    "print(f'AVG chunks per speaker: {round(num_chunks/counter, 2)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-freevc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
