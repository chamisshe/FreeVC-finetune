{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning FreeVC\n",
    "\n",
    "\n",
    "Hi!\n",
    "\n",
    "This notebook takes you through the steps to **fine-tune** the **Voice Conversion** model **FreeVC**. It is meant to be beginner-friendly, sparing you (and myself, the author of this notebook) most of the details of FreeVC's incredibly complicated Architecture. To those looking for a deeper dive into some of the concepts, models and techniques on which FreeVC is built, there will be some links to further reading.\n",
    "\n",
    "> **FreeVC: [Paper](https://arxiv.org/abs/2210.15418) | [Demo](https://olawod.github.io/FreeVC-demo/) | [Code](https://github.com/OlaWod/FreeVC)**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove before shipping\n",
    "# ! python initialize.py --leave_chunks\n",
    "# DO_NOT_CHUNK = True\n",
    "# DO_NOT_CHUNK = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "### What is Voice Conversion?\n",
    "\n",
    "Voice conversion is probably best explained using an example: There are two people, Alice and Bob. Alice wants to impersonate Bob, and she has a recording of Bob's voice. Alice then makes recordings of herself, saying things Bob would never say, such as \"give lots of money to Alice\". Using the recording she has of Bob, she *converts* the voice to sounding like Bob is saying all these ridiculous things.\n",
    "\n",
    "In this scenario, Alice is the **source** and Bob is the **target voice**. The distinction may seem a bit arbitrary at first, since we need both a recording of Alice *and* Bob, but since Bob's voice is where we want to end up, this distinction makes sense.\n",
    "\n",
    "#### Intuition\n",
    "Without going into technical details, let's ask why this works *intuitively*.\n",
    "\n",
    "When we listen to a voice, our brains more or less automatically process two different things:\n",
    "- *Who is speaking?* (let's call that **identity** or **speaker information**)\n",
    "- *What is being said?* (let's call that **content**)\n",
    "Determining the identity comes down to factors both physically inherent to your voice - mainly timbre and pitch - as well as factors that are more under the speaker's direct control, things like accent, rhythm.\n",
    "\n",
    "The content is largely independent of the features that make up identity. Communication works because people with different voices are able to produce the same phonemes. The same sequence of vowels and consonants - be it a word, sentence or speech - means the same thing across different speakers.\n",
    "\n",
    "The identity and content information on a *signal* (a spoken utterance) seem to be independent of each other, to a certain degree. The core idea for voice conversion therefore is the following:\n",
    "1. Given a source signal (by speaker A), strip it all its speaker information, while preserving content. \n",
    "2. Extract target speaker information (speaker B).\n",
    "3. Insert target speaker information into the stripped source signal\n",
    "\n",
    "<details>\n",
    "\n",
    "**<summary>Difference to Voice Cloning</summary>**\n",
    "\n",
    "How does voice conversion differ from voice cloning?\n",
    "\n",
    "Fundamentally, voice cloning - the extracting of speaker features and using them to generate speech - falls in the realm of text-to-speech, whilst voice conversion is speech-to-speech and entirely textless. There are also some significant practical differences: For instance, sophisticated voice cloning models also preserve properties such as accent and rhythm, whereas voice conversion does not. The stripped signal in voice conversion is much more of a rigid template than the text input used in voice cloning. The advantage of this is that it allows the user more direct control over the rhythm, accent, pitch contour and so on, simply by having the desired patterns in the source signal.\n",
    "\n",
    "</details>\n",
    "<!-- such as timbre (the inherent sound of your voice - German has the beautiful word *Klangfarbe* for it, which literally translates to \"Sound colour\"), pitch, -->\n",
    "<!-- The goal of voice conversion is to make a recorded utterance by *speaker A* sound like it's being said by *speaker B*. To do so, we essentially want to remove the speaker-specific features of speaker A from the recording, and replace them with speaker B's features. This is different from similar technology such as voice cloning, which achieves a similar output, but is still text-to-speech. -->\n",
    "\n",
    "### FreeVC: Architecture\n",
    "\n",
    "A significant part of FreeVC's architecture is based on [**VITS**](https://github.com/jaywalnut310/vits), an **end-to-end text-to-speech** model. VITS is a popular model because its output sounds very human for a TTS system. A core piece of VITS is the [Conditional Variational Autoencoder](https://theaiacademy.blogspot.com/2020/05/understanding-conditional-variational.html), a type of autoencoder more suitable for d\n",
    " However, as a TTS system it uses text as the input, whereas voice conversion is a speech-to-speech task. Roughly summarized and glossing over many technical details, FreeVC modifies the architecture of VITS in two ways: \n",
    "\n",
    "Firstly, it replaces the text encoder with an encoder capable of handling speech. The encoder itself consists of multiple pieces: A pretrained model that transforms the raw waveform into a vector (WavLM), a *bottleneck extractor* that reduces the dimensionality of the obtained vector and hopefully sieves out the information not needed, and lastly a normalizing flow (read more on flow [here](https://towardsdatascience.com/introduction-to-normalizing-flows-d002af262a4b)).\n",
    "\n",
    "Secondly, it adds a pretrained **speaker encoder** to the model architecture. A mel-spectrogram of the audio sample of the target speaker is given as input to the speaker encoder. It extracts the relevant speaker features, and feeds the resulting *speaker embedding* into both the flow module and the decoder.\n",
    "\n",
    "Finally, the decoder takes the encoded source audio and the speaker embedding, and creates the output waveform from it. As in VITS, FreeVC also uses [HiFi-GAN V1](https://github.com/jik876/hifi-gan) as its decoder.\n",
    "\n",
    "Additionally, FreeVC uses a discriminator to incorporate [adversarial learning](https://developers.google.com/machine-learning/gan/) as well as data augmentation during training of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "\n",
    "\n",
    "### Installation/Prerequisites\n",
    "- Python 3.9.19\n",
    "- WavLM\n",
    "- HiFiGAN\n",
    "- venv:\n",
    "    - `pip install -r requirements.txt`\n",
    "    - TODO: ensure the following packages are included in the requirements\n",
    "        - protobuf<=3.20.3\n",
    "        - six==1.16.0\n",
    "        - matplotlib\n",
    "        - numpy<=1.22.4\n",
    "- ffmpeg: to enable exporting as flac\n",
    "    - `sudo apt update && sudo apt upgrade` `sudo apt install ffmpeg`\n",
    "    - `ffmpeg -version` to check the installation \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Venv & install requirements\n",
    "\n",
    "\n",
    "#### Ensure Prerequisites\n",
    "Firstly, make sure that Python 3.9 and FFmpeg are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Python 3.9\n",
    "! python3.9 -V\n",
    "\n",
    "# Check for FFmpeg\n",
    "! ffmpeg --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "**<summary>Install Python 3.9 & FFmpeg</summary>**\n",
    "\n",
    "If checking with the above commands gives you an error, follow these steps:\n",
    "\n",
    "##### Python 3.9\n",
    "```bash\n",
    "sudo add-apt-repository ppa:deadsnakes/ppa\n",
    "sudo apt-get update && sudo apt upgrade\n",
    "sudo apt-get install python3.9\n",
    "```\n",
    "\n",
    "##### FFmpeg\n",
    "```bash\n",
    "sudo apt update\n",
    "sudo apt install ffmpeg\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Venv & Packages\n",
    "\n",
    "Next, as is standard procedure, we want to install the required modules inside a Virtual Environment (or venv). Because different projects have different dependencies, we want to keep them from interfering with each other, therefore we install the required dependencies in an isolated environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create venv\n",
    "! python3.9 -m venv .venv-freevc\n",
    "# activate venv\n",
    "! source .venv-freevc/bin/activate\n",
    "# install requirements\n",
    "! pip install -r requirements.txt\n",
    "# ! pip install ipykernel\n",
    "# add the venv to the registry of jupyter kernels, allowing us to use  allows jupyter \n",
    "! python3.9 -m ipykernel install --name=.venv-freevc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WavLM\n",
    "\n",
    "Download the `WavLM Large` model found on [this page](https://github.com/microsoft/unilm/tree/master/wavlm).\n",
    "> `Pre-Trained Models` > WavLM Large `Google Drive`\n",
    "\n",
    "Next, move the downloaded file into the `wavlm/` folder.\n",
    "\n",
    "You'll want to end up with the following folder structure:\n",
    "> ```ascii\n",
    "> FreeVC-finetune/\n",
    "> ├─ ...\n",
    "> ├─ wavlm/\n",
    "> │  ├─ modules.py/\n",
    "> │  ├─ WavLM-Large.pt\n",
    "> │  ├─ WavLM-Large.pt.txt\n",
    "> │  ├─ WavLM.py\n",
    "> │  ├─ __init__.py\n",
    "> ├─ ...\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HiFi-GAN\n",
    "Download the HiFi-GAN model `VCTK_V1` found on [this page](https://github.com/jik876/hifi-gan?tab=readme-ov-file). \n",
    "> `Download Pretrained Models` > Google Drive: `VCTK_V1` > Download `generator_v1`\n",
    "\n",
    "Next, move the downloaded file into the `hifigan/` folder.\n",
    "\n",
    "You'll want to end up with the following folder structure:\n",
    "> ```ascii\n",
    "> FreeVC-finetune/\n",
    "> ├─ ...\n",
    "> ├─ hifigan/\n",
    "> │  ├─ __init__.py\n",
    "> │  ├─ config.json\n",
    "> │  ├─ generator_v1\n",
    "> │  ├─ generator_v1.txt\n",
    "> │  ├─ models.py/\n",
    "> ├─ ...\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the user followed the previous two steps correctly\n",
    "import os\n",
    "file = \"wavlm/WavLM-Large.pt\"\n",
    "assert os.path.exists(file), f\"{file} is missing.\\nMake sure you downloaded the WavLM-Large model and put it in the correct directory\"\n",
    "file = \"hifigan/generator_v1\"\n",
    "assert os.path.exists(file), f\"{file} is missing.\\nMake sure you downloaded the HiFi-Gan model (VCTK_V1) and put it in the correct directory\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Preparation\n",
    "\n",
    "To finetune the pre-trained model, we of course need some training data to adapt the model to the target speaker. With _some_ training data, I mean a whole lot of it.\n",
    "\n",
    "We want to make sure we make the most of our data. Therefore, we'll do some simple preprocessing on it.\n",
    "\n",
    "#### Chop it up\n",
    "\n",
    "The **base model** (i.e. FreeVC's pretrained model that we're finetuning) is trained on the [VCTK corpus](https://datashare.ed.ac.uk/handle/10283/3443). The audio in this corpus is stored in multiple smaller files (3.4 seconds on average) than one large file.\n",
    "\n",
    "If your finetuning-data is already in similarly sized chunks, you can **skip this step**. Otherwise, run the following cells on your file(s): This will automatically detect silent passages - for example between sentences, and thus split your audio into adequately sized chunks.\n",
    "\n",
    "You can modify the following parameters to control the chunking:\n",
    "- `MIN_SILENCE_LEN` (miliseconds): Defines the minimal length of silence necessary to split the audio at that point\n",
    "- `SILENCE_THRESHOLD` (dBFS): Defines what counts as silent and what does not; anything louder than the set threshold will count as not silent. \n",
    "- `MIN_CHUNK_LEN` (seconds): Any chunk shorter than this value will be discarded and NOT saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified code, originally from:\n",
    "#   https://www.codespeedy.com/split-audio-files-using-silence-detection-in-python/\n",
    "#   retrieved on 2024-08-23\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "\n",
    "def chunk_audio(filelist: list[str],silence_len=800,silence_thr=-40, chunklen=0):\n",
    "    count = 0\n",
    "    length = 0.\n",
    "    for file in filelist:\n",
    "        sound = AudioSegment.from_wav(file)\n",
    "        # print(sound.duration_seconds)\n",
    "        # spliting audio files\n",
    "        audio_chunks = split_on_silence(sound, min_silence_len=silence_len, silence_thresh=silence_thr)\n",
    "        #loop is used to iterate over the output list\n",
    "        for i, chunk in enumerate(audio_chunks):\n",
    "            # save them as a FLAC file\n",
    "            output_file = \"./chunks/chunk{0}.flac\".format(i)\n",
    "            if not os.path.exists(\"./chunks\"):\n",
    "                os.makedirs(\"./chunks/\")\n",
    "            if chunk.duration_seconds >= chunklen:\n",
    "                print(\"Exporting file\", output_file)\n",
    "                chunk.export(output_file, format=\"flac\")\n",
    "                length += chunk.duration_seconds\n",
    "                count += 1\n",
    "            else:\n",
    "                print(\"Skipping Chunk {0}: Too short (< {1} seconds)\".format(i,chunklen))\n",
    "    print(\"\\nAverage length of saved chunks: {0} Seconds\".format(round(length/count,2)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Chunk 0: Too short (< 1.5 seconds)\n",
      "Skipping Chunk 1: Too short (< 1.5 seconds)\n",
      "Exporting file ./chunks/chunk2.flac\n",
      "Exporting file ./chunks/chunk3.flac\n",
      "Exporting file ./chunks/chunk4.flac\n",
      "Exporting file ./chunks/chunk5.flac\n",
      "Exporting file ./chunks/chunk6.flac\n",
      "Exporting file ./chunks/chunk7.flac\n",
      "Exporting file ./chunks/chunk8.flac\n",
      "Exporting file ./chunks/chunk9.flac\n",
      "Skipping Chunk 10: Too short (< 1.5 seconds)\n",
      "Exporting file ./chunks/chunk11.flac\n",
      "Exporting file ./chunks/chunk12.flac\n",
      "Exporting file ./chunks/chunk13.flac\n",
      "Exporting file ./chunks/chunk14.flac\n",
      "Exporting file ./chunks/chunk15.flac\n",
      "Exporting file ./chunks/chunk16.flac\n",
      "Exporting file ./chunks/chunk17.flac\n",
      "Exporting file ./chunks/chunk18.flac\n",
      "Skipping Chunk 19: Too short (< 1.5 seconds)\n",
      "Skipping Chunk 20: Too short (< 1.5 seconds)\n",
      "Exporting file ./chunks/chunk21.flac\n",
      "Skipping Chunk 22: Too short (< 1.5 seconds)\n",
      "Exporting file ./chunks/chunk23.flac\n",
      "Exporting file ./chunks/chunk24.flac\n",
      "Skipping Chunk 25: Too short (< 1.5 seconds)\n",
      "Exporting file ./chunks/chunk26.flac\n",
      "Skipping Chunk 27: Too short (< 1.5 seconds)\n",
      "Exporting file ./chunks/chunk28.flac\n",
      "Skipping Chunk 29: Too short (< 1.5 seconds)\n",
      "Exporting file ./chunks/chunk30.flac\n",
      "Exporting file ./chunks/chunk31.flac\n",
      "Skipping Chunk 32: Too short (< 1.5 seconds)\n",
      "Exporting file ./chunks/chunk33.flac\n",
      "Exporting file ./chunks/chunk34.flac\n",
      "Exporting file ./chunks/chunk35.flac\n",
      "Skipping Chunk 36: Too short (< 1.5 seconds)\n",
      "Skipping Chunk 37: Too short (< 1.5 seconds)\n",
      "Exporting file ./chunks/chunk38.flac\n",
      "Exporting file ./chunks/chunk39.flac\n",
      "Exporting file ./chunks/chunk40.flac\n",
      "Exporting file ./chunks/chunk41.flac\n",
      "Exporting file ./chunks/chunk42.flac\n",
      "Exporting file ./chunks/chunk43.flac\n",
      "Exporting file ./chunks/chunk44.flac\n",
      "Exporting file ./chunks/chunk45.flac\n",
      "Exporting file ./chunks/chunk46.flac\n",
      "Exporting file ./chunks/chunk47.flac\n",
      "Exporting file ./chunks/chunk48.flac\n",
      "Skipping Chunk 49: Too short (< 1.5 seconds)\n",
      "Exporting file ./chunks/chunk50.flac\n",
      "Exporting file ./chunks/chunk51.flac\n",
      "Exporting file ./chunks/chunk52.flac\n",
      "Exporting file ./chunks/chunk53.flac\n",
      "Exporting file ./chunks/chunk54.flac\n",
      "Exporting file ./chunks/chunk55.flac\n",
      "Exporting file ./chunks/chunk56.flac\n",
      "Skipping Chunk 57: Too short (< 1.5 seconds)\n",
      "Exporting file ./chunks/chunk58.flac\n",
      "Exporting file ./chunks/chunk59.flac\n",
      "Exporting file ./chunks/chunk60.flac\n",
      "Exporting file ./chunks/chunk61.flac\n",
      "Skipping Chunk 62: Too short (< 1.5 seconds)\n",
      "Skipping Chunk 63: Too short (< 1.5 seconds)\n",
      "Exporting file ./chunks/chunk64.flac\n",
      "Exporting file ./chunks/chunk65.flac\n",
      "Exporting file ./chunks/chunk66.flac\n",
      "Exporting file ./chunks/chunk67.flac\n",
      "Skipping Chunk 68: Too short (< 1.5 seconds)\n",
      "\n",
      "Average length of saved chunks: 4.05 Seconds\n"
     ]
    }
   ],
   "source": [
    "# VARIABLES\n",
    "MIN_SILENCE_LEN = 800\n",
    "SILENCE_THRESHOLD = -40\n",
    "MIN_CHUNK_LEN = 1.5\n",
    "\n",
    "# USER TODO: list of files to chunk\n",
    "filelist = [\"./LN_AUDIOFILES/brn1/bernie_filibuster_pt1_5min.wav\"]\n",
    "\n",
    "# TODO remove before shipping\n",
    "if not DO_NOT_CHUNK:\n",
    "    chunk_audio(filelist, silence_len=MIN_SILENCE_LEN, silence_thr=SILENCE_THRESHOLD, chunklen=MIN_CHUNK_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "At this point, we have some audio data in appropriately sized chunks. We now need to run some very particular preprocessing steps on it, so that the model receives it in the right format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storage Format\n",
    "\n",
    "FreeVC expects our fine-tuning data to be in the same format as its [original training data](https://datashare.ed.ac.uk/handle/10283/3443). Therefore, we need to rename some files and move them to the right places before we run any preprocessing.\n",
    "\n",
    "You'll need to assign some **4-character** ID to your speaker - pick one that makes sense to you. If it's longer or shorter than 4 characters, this won't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: pick your Speaker ID\n",
    "SPEAKER_ID = \"brn1\"\n",
    "assert len(SPEAKER_ID) == 4\n",
    "\n",
    "# DIRECTORY NAMES\n",
    "CHUNKS = \"./chunks/\"\n",
    "FLACS = \"./dataset/flac/\"\n",
    "DATA_PATH = f'{FLACS}{SPEAKER_ID}/'\n",
    "DATA16K = \"dataset/finetuning-16k\"\n",
    "DATA22K = \"dataset/finetuning-22k\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will rename your audio and move it into a directory with the right structure.\n",
    "\n",
    "`<some_dir>/<sp_id>/<sp_id-filename>_mic2.flac`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved and renamed your training files.\n",
      "Great Success!! Very Nice!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    os.makedirs(DATA_PATH)\n",
    "\n",
    "for i,file in enumerate(os.listdir(CHUNKS)):\n",
    "    # print(file)\n",
    "    # rename & move files to the specific format necessary\n",
    "    new_filename = f'{SPEAKER_ID}-{i}_mic2.flac'\n",
    "    a =os.path.join(CHUNKS,file)\n",
    "    shutil.copy(a, os.path.join(DATA_PATH,new_filename))\n",
    "print(\"Moved and renamed your training files.\\nGreat Success!! Very Nice!\")\n",
    "# os.listdir(DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downsampling\n",
    "\n",
    "Downsamples the audio to 16kHz.\n",
    "- `--sr1` sampling rate`\n",
    "- `--sr2` sampling rate`\n",
    "- `--in_dir` path to source dir`\n",
    "- `--out_dir1` path to target dir`\n",
    "- `--out_dir2` path to target dir`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python downsample.py --in_dir $FLACS --out_dir1 $DATA16K --out_dir2 $DATA22K\n",
    "! ln -s $DATA16K DUMMY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Splitting\n",
    "\n",
    "Next, our fine-tuning data will need to be split into a training, test and validation set.\n",
    "\n",
    "The original splitting-script of FreeVC uses 2 chunks from each speaker for validation, 10 chunks for testing and the rest for training. With an average of around 400 chunks per speaker, this is an average test-split of 2.5%, and validation-split of 0.5%. To me, this seems like an overly small test and validation portion.\n",
    "\n",
    "Therefore the preprocessing script was modified:\n",
    "Before, the test and validation portions were constant, at 10 and 2 samples respectively. I changed them to a relative 5% and 1% portion for the test and validation sets.\n",
    "\n",
    "_(As to whether this improves or worsens performance, I have no empirical evidence for either and I do not intend to gather it.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 5178.15it/s]\n",
      "Writing ./filelists/finetune-train.txt\n",
      "100%|███████████████████████████████████████| 48/48 [00:00<00:00, 182196.01it/s]\n",
      "Writing ./filelists/finetune-val.txt\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 14873.42it/s]\n",
      "Writing ./filelists/finetune-test.txt\n",
      "100%|██████████████████████████████████████████| 3/3 [00:00<00:00, 35444.82it/s]\n"
     ]
    }
   ],
   "source": [
    "val_file=\"./filelists/finetune-val.txt\"\n",
    "test_file=\"./filelists/finetune-test.txt\"\n",
    "train_file=\"./filelists/finetune-train.txt\"\n",
    "sr_wavs = f\"./dataset/sr/wav\"\n",
    "# ! python preprocess_flist.py --train_list $train_file --test_list $test_file --val_list $val_file --source_dir $sr_wavs\n",
    "! python preprocess_flist.py --train_list $train_file --test_list $test_file --val_list $val_file --source_dir $DATA16K\n",
    "! rm DUMMY\n",
    "! ln -s $DATA16K DUMMY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$train_file\n"
     ]
    }
   ],
   "source": [
    "# %%bash -s \"$train_file\" \"$test_file\" \"$val_file\" \"$DATA16K\"\n",
    "# echo $1\n",
    "# python preprocess_flist.py --train_list $1 --test_list $2 --val_list $3 --source_dir $4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speaker Encoder (pretrained)\n",
    "\n",
    "Something something encode speaker information using a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root=\"./dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$DATA16K\n",
      "this is $data_root\n"
     ]
    }
   ],
   "source": [
    "! CUDA_VISIBLE_DEVICES=0 python preprocess_spk.py --in_dir $DATA16K --out_dir_root $data_root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Augmentation\n",
    "\n",
    "To make the most of our data...    ...Spectrogram Resize (SR)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIFIGAN_CFG = \"hifigan/config.json\"\n",
    "WAV_DIR = \"dataset/sr/wav\"\n",
    "SSL_DIR = \"dataset/sr/wavlm\"\n",
    "! CUDA_VISIBLE_DEVICES=0 python preprocess_sr.py --in_dir $DATA22K --wav_dir $WAV_DIR --ssl_dir $SSL_DIR --config $HIFIGAN_CFG --min 68 --max 92 --sr 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning\n",
    "\n",
    "The hyperparameters of training are set in a JSON file, located in the `/configs/` directory. For finetuning, we'll use the file `freevc-finetune.json`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freevc_finetune\n",
      "INFO:freevc-finetune:{'train': {'log_interval': 10, 'eval_interval': 10, 'seed': 1234, 'epochs': 2, 'learning_rate': 0.0002, 'betas': [0.8, 0.99], 'eps': 1e-09, 'batch_size': 1, 'fp16_run': False, 'lr_decay': 0.999875, 'segment_size': 8960, 'init_lr_ratio': 1, 'warmup_epochs': 0, 'c_mel': 45, 'c_kl': 1.0, 'use_sr': True, 'max_speclen': 128, 'port': '8001'}, 'data': {'training_files': 'filelists/finetune-train.txt', 'validation_files': 'filelists/finetune-val.txt', 'max_wav_value': 32768.0, 'sampling_rate': 16000, 'filter_length': 1280, 'hop_length': 320, 'win_length': 1280, 'n_mel_channels': 80, 'mel_fmin': 0.0, 'mel_fmax': None}, 'model': {'inter_channels': 192, 'hidden_channels': 192, 'filter_channels': 768, 'n_heads': 2, 'n_layers': 6, 'kernel_size': 3, 'p_dropout': 0.1, 'resblock': '1', 'resblock_kernel_sizes': [3, 7, 11], 'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'upsample_rates': [10, 8, 2, 2], 'upsample_initial_channel': 512, 'upsample_kernel_sizes': [16, 16, 4, 4], 'n_layers_q': 3, 'use_spectral_norm': False, 'gin_channels': 256, 'ssl_dim': 1024, 'use_spk': True}, 'model_dir': './logs/freevc-finetune', 'generator': './checkpoints/freevc.pth', 'discriminator': './checkpoints/D-freevc.pth', 'force_new': True}\n",
      "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n",
      "train_loader:  48\n",
      "INFO:freevc-finetune:Loaded checkpoint './checkpoints/freevc.pth' (iteration 1372)\n",
      "INFO:freevc-finetune:Loaded checkpoint './checkpoints/D-freevc.pth' (iteration 1372)\n",
      "filename:  DUMMY/brn1/brn1-8.wav\n",
      "filename:  DUMMY/brn1/brn1-16.wav\n",
      "filename:  DUMMY/brn1/brn1-32.wav\n",
      "filename:  DUMMY/brn1/brn1-14.wav\n",
      "filename:  DUMMY/brn1/brn1-28.wav\n",
      "filename:  DUMMY/brn1/brn1-27.wav\n",
      "filename:  DUMMY/brn1/brn1-5.wav\n",
      "filename:  DUMMY/brn1/brn1-3.wav\n",
      "filename:  DUMMY/brn1/brn1-51.wav\n",
      "filename:  DUMMY/brn1/brn1-45.wav\n",
      "filename:  DUMMY/brn1/brn1-36.wav\n",
      "filename:  DUMMY/brn1/brn1-41.wav\n",
      "filename:  DUMMY/brn1/brn1-44.wav\n",
      "filename:  DUMMY/brn1/brn1-19.wav\n",
      "filename:  DUMMY/brn1/brn1-17.wav\n",
      "filename:  DUMMY/brn1/brn1-21.wav\n",
      "filename:  DUMMY/brn1/brn1-42.wav\n",
      "INFO:root:Reducer buckets have been rebuilt in this iteration.\n",
      "INFO:freevc-finetune:Train Epoch: 1 [0%]\n",
      "INFO:freevc-finetune:[2.010392904281616, 6.994058609008789, 14.77212905883789, 26.200119018554688, 60.47795867919922, 0, 0.00016845718778664938]\n",
      "DEBUG:matplotlib:matplotlib data path: /home/notmyyka/UZH/pp_vc/FreeVC/.venv-freevc/lib/python3.9/site-packages/matplotlib/mpl-data\n",
      "DEBUG:matplotlib:CONFIGDIR=/home/notmyyka/.config/matplotlib\n",
      "DEBUG:matplotlib:interactive is False\n",
      "DEBUG:matplotlib:platform is linux\n",
      "filename:  DUMMY/brn1/brn1-9.wav\n",
      "INFO:freevc-finetune:Saving model and optimizer state at iteration 1 to ./logs/freevc-finetune/G_0.pth\n",
      "INFO:freevc-finetune:Saving model and optimizer state at iteration 1 to ./logs/freevc-finetune/D_0.pth\n",
      "filename:  DUMMY/brn1/brn1-10.wav\n",
      "INFO:root:Reducer buckets have been rebuilt in this iteration.\n",
      "filename:  DUMMY/brn1/brn1-23.wav\n",
      "filename:  DUMMY/brn1/brn1-20.wav\n",
      "filename:  DUMMY/brn1/brn1-24.wav\n",
      "filename:  DUMMY/brn1/brn1-15.wav\n",
      "filename:  DUMMY/brn1/brn1-38.wav\n",
      "filename:  DUMMY/brn1/brn1-48.wav\n",
      "filename:  DUMMY/brn1/brn1-25.wav\n",
      "filename:  DUMMY/brn1/brn1-50.wav\n",
      "filename:  DUMMY/brn1/brn1-46.wav\n",
      "INFO:freevc-finetune:Train Epoch: 1 [21%]\n",
      "INFO:freevc-finetune:[4.3742852210998535, 4.717919826507568, 13.515385627746582, 32.10834884643555, 6.6031928062438965, 10, 0.00016845718778664938]\n",
      "filename:  DUMMY/brn1/brn1-9.wav\n",
      "INFO:freevc-finetune:Saving model and optimizer state at iteration 1 to ./logs/freevc-finetune/G_10.pth\n",
      "INFO:freevc-finetune:Saving model and optimizer state at iteration 1 to ./logs/freevc-finetune/D_10.pth\n",
      "filename:  DUMMY/brn1/brn1-29.wav\n",
      "filename:  DUMMY/brn1/brn1-2.wav\n",
      "filename:  DUMMY/brn1/brn1-49.wav\n",
      "filename:  DUMMY/brn1/brn1-30.wav\n",
      "filename:  DUMMY/brn1/brn1-6.wav\n",
      "filename:  DUMMY/brn1/brn1-31.wav\n",
      "filename:  DUMMY/brn1/brn1-43.wav\n",
      "filename:  DUMMY/brn1/brn1-13.wav\n",
      "filename:  DUMMY/brn1/brn1-0.wav\n",
      "filename:  DUMMY/brn1/brn1-22.wav\n",
      "INFO:freevc-finetune:Train Epoch: 1 [42%]\n",
      "INFO:freevc-finetune:[2.787867546081543, 3.013143539428711, 10.779343605041504, 26.900354385375977, 6.112410545349121, 20, 0.00016845718778664938]\n",
      "filename:  DUMMY/brn1/brn1-9.wav\n",
      "INFO:freevc-finetune:Saving model and optimizer state at iteration 1 to ./logs/freevc-finetune/G_20.pth\n",
      "INFO:freevc-finetune:Saving model and optimizer state at iteration 1 to ./logs/freevc-finetune/D_20.pth\n",
      "filename:  DUMMY/brn1/brn1-26.wav\n",
      "filename:  DUMMY/brn1/brn1-33.wav\n",
      "filename:  DUMMY/brn1/brn1-37.wav\n",
      "filename:  DUMMY/brn1/brn1-35.wav\n",
      "filename:  DUMMY/brn1/brn1-11.wav\n",
      "filename:  DUMMY/brn1/brn1-12.wav\n",
      "filename:  DUMMY/brn1/brn1-47.wav\n",
      "filename:  DUMMY/brn1/brn1-40.wav\n",
      "filename:  DUMMY/brn1/brn1-4.wav\n",
      "filename:  DUMMY/brn1/brn1-34.wav\n",
      "INFO:freevc-finetune:Train Epoch: 1 [62%]\n",
      "INFO:freevc-finetune:[2.717712640762329, 3.2056026458740234, 9.663077354431152, 29.52743911743164, 5.3559956550598145, 30, 0.00016845718778664938]\n",
      "filename:  DUMMY/brn1/brn1-9.wav\n",
      "INFO:freevc-finetune:Saving model and optimizer state at iteration 1 to ./logs/freevc-finetune/G_30.pth\n",
      "INFO:freevc-finetune:Saving model and optimizer state at iteration 1 to ./logs/freevc-finetune/D_30.pth\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "INFO:freevc-finetune:Train Epoch: 1 [83%]\n",
      "INFO:freevc-finetune:[2.5944182872772217, 3.653904438018799, 9.949706077575684, 25.58915901184082, 3.5754499435424805, 40, 0.00016845718778664938]\n",
      "filename:  DUMMY/brn1/brn1-9.wav\n",
      "INFO:freevc-finetune:Saving model and optimizer state at iteration 1 to ./logs/freevc-finetune/G_40.pth\n",
      "INFO:freevc-finetune:Saving model and optimizer state at iteration 1 to ./logs/freevc-finetune/D_40.pth\n",
      "INFO:freevc-finetune:====> Epoch: 1\n",
      "filename:  DUMMY/brn1/brn1-6.wav\n",
      "filename:  DUMMY/brn1/brn1-41.wav\n",
      "filename:  DUMMY/brn1/brn1-51.wav\n",
      "filename:  DUMMY/brn1/brn1-4.wav\n",
      "filename:  DUMMY/brn1/brn1-1.wav\n",
      "filename:  DUMMY/brn1/brn1-21.wav\n",
      "filename:  DUMMY/brn1/brn1-15.wav\n",
      "filename:  DUMMY/brn1/brn1-24.wav\n",
      "filename:  DUMMY/brn1/brn1-25.wav\n",
      "filename:  DUMMY/brn1/brn1-0.wav\n",
      "filename:  DUMMY/brn1/brn1-19.wav\n",
      "filename:  DUMMY/brn1/brn1-8.wav\n",
      "filename:  DUMMY/brn1/brn1-23.wav\n",
      "filename:  DUMMY/brn1/brn1-10.wav\n",
      "filename:  DUMMY/brn1/brn1-14.wav\n",
      "filename:  DUMMY/brn1/brn1-47.wav\n",
      "filename:  DUMMY/brn1/brn1-40.wav\n",
      "filename:  DUMMY/brn1/brn1-17.wav\n",
      "filename:  DUMMY/brn1/brn1-30.wav\n",
      "INFO:freevc-finetune:Train Epoch: 2 [4%]\n",
      "INFO:freevc-finetune:[2.669412851333618, 2.9398128986358643, 8.301678657531738, 24.379858016967773, 4.999719142913818, 50, 0.00016843613063817603]\n",
      "filename:  DUMMY/brn1/brn1-9.wav\n",
      "INFO:freevc-finetune:Saving model and optimizer state at iteration 2 to ./logs/freevc-finetune/G_50.pth\n",
      "INFO:freevc-finetune:Saving model and optimizer state at iteration 2 to ./logs/freevc-finetune/D_50.pth\n",
      "filename:  DUMMY/brn1/brn1-45.wav\n",
      "filename:  DUMMY/brn1/brn1-5.wav\n",
      "filename:  DUMMY/brn1/brn1-22.wav\n",
      "filename:  DUMMY/brn1/brn1-29.wav\n",
      "filename:  DUMMY/brn1/brn1-42.wav\n",
      "filename:  DUMMY/brn1/brn1-31.wav\n",
      "filename:  DUMMY/brn1/brn1-3.wav\n",
      "filename:  DUMMY/brn1/brn1-32.wav\n",
      "filename:  DUMMY/brn1/brn1-49.wav\n",
      "filename:  DUMMY/brn1/brn1-20.wav\n",
      "INFO:freevc-finetune:Train Epoch: 2 [25%]\n",
      "INFO:freevc-finetune:[2.6520676612854004, 2.9294228553771973, 9.034720420837402, 24.587329864501953, 3.212935447692871, 60, 0.00016843613063817603]\n",
      "filename:  DUMMY/brn1/brn1-9.wav\n",
      "INFO:freevc-finetune:Saving model and optimizer state at iteration 2 to ./logs/freevc-finetune/G_60.pth\n",
      "INFO:freevc-finetune:Saving model and optimizer state at iteration 2 to ./logs/freevc-finetune/D_60.pth\n",
      "filename:  DUMMY/brn1/brn1-27.wav\n",
      "filename:  DUMMY/brn1/brn1-38.wav\n",
      "filename:  DUMMY/brn1/brn1-36.wav\n",
      "filename:  DUMMY/brn1/brn1-44.wav\n",
      "filename:  DUMMY/brn1/brn1-28.wav\n",
      "filename:  DUMMY/brn1/brn1-33.wav\n",
      "filename:  DUMMY/brn1/brn1-46.wav\n",
      "filename:  DUMMY/brn1/brn1-2.wav\n",
      "filename:  DUMMY/brn1/brn1-35.wav\n",
      "filename:  DUMMY/brn1/brn1-37.wav\n",
      "INFO:freevc-finetune:Train Epoch: 2 [46%]\n",
      "INFO:freevc-finetune:[2.1860549449920654, 2.5580296516418457, 5.534112930297852, 26.266923904418945, 2.5229082107543945, 70, 0.00016843613063817603]\n",
      "filename:  DUMMY/brn1/brn1-9.wav\n",
      "INFO:freevc-finetune:Saving model and optimizer state at iteration 2 to ./logs/freevc-finetune/G_70.pth\n",
      "INFO:freevc-finetune:Saving model and optimizer state at iteration 2 to ./logs/freevc-finetune/D_70.pth\n",
      "filename:  DUMMY/brn1/brn1-13.wav\n",
      "filename:  DUMMY/brn1/brn1-26.wav\n",
      "filename:  DUMMY/brn1/brn1-50.wav\n",
      "filename:  DUMMY/brn1/brn1-16.wav\n",
      "filename:  DUMMY/brn1/brn1-34.wav\n",
      "filename:  DUMMY/brn1/brn1-48.wav\n",
      "filename:  DUMMY/brn1/brn1-12.wav\n",
      "filename:  DUMMY/brn1/brn1-43.wav\n",
      "filename:  DUMMY/brn1/brn1-11.wav\n",
      "INFO:freevc-finetune:Train Epoch: 2 [67%]\n",
      "INFO:freevc-finetune:[2.9834413528442383, 2.4088377952575684, 10.471174240112305, 25.31476402282715, 2.9278931617736816, 80, 0.00016843613063817603]\n",
      "filename:  DUMMY/brn1/brn1-9.wav\n",
      "INFO:freevc-finetune:Saving model and optimizer state at iteration 2 to ./logs/freevc-finetune/G_80.pth\n",
      "INFO:freevc-finetune:Saving model and optimizer state at iteration 2 to ./logs/freevc-finetune/D_80.pth\n",
      "INFO:freevc-finetune:Train Epoch: 2 [88%]\n",
      "INFO:freevc-finetune:[2.238560676574707, 2.5140392780303955, 6.960983753204346, 21.335159301757812, 5.208609580993652, 90, 0.00016843613063817603]\n",
      "filename:  DUMMY/brn1/brn1-9.wav\n",
      "INFO:freevc-finetune:Saving model and optimizer state at iteration 2 to ./logs/freevc-finetune/G_90.pth\n",
      "INFO:freevc-finetune:Saving model and optimizer state at iteration 2 to ./logs/freevc-finetune/D_90.pth\n",
      "INFO:freevc-finetune:====> Epoch: 2\n"
     ]
    }
   ],
   "source": [
    "# train freevc: use config 'configs/freevc-finetune.json', use model 'freevc'\n",
    "# ! CUDA_VISIBLE_DEVICES=0 python finetune.py --config configs/freevc-finetune.json --model freevc-finetune\n",
    "MODEL_NAME = f'freevc_finetune-{SPEAKER_ID}'\n",
    "MODEL_NAME = f'freevc_finetune'\n",
    "! echo $MODEL_NAME\n",
    "! CUDA_VISIBLE_DEVICES=0 python finetune.py -c configs/freevc-finetune.json -m freevc-finetune -d ./checkpoints/D-freevc.pth -g ./checkpoints/freevc.pth --force_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Loading checkpoint...\n",
      "logs/freevc-finetune/G_40.pth\n",
      "INFO:root:Loaded checkpoint 'logs/freevc-finetune/G_40.pth' (iteration 1)\n",
      "Loading WavLM for content...\n",
      "INFO:wavlm.WavLM:WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}\n",
      "Loading speaker encoder...\n",
      "Loaded the voice encoder model on cuda in 0.08 seconds.\n",
      "Processing text...\n",
      "Synthesizing...\n",
      "2it [00:06,  3.04s/it]\n"
     ]
    }
   ],
   "source": [
    "# ! CUDA_VISIBLE_DEVICES=0 python convert.py --hpfile configs/freevc-finetune.json --ptfile checkpoints/freevc-finetune.pth --txtpath convert.txt --outdir outputs/freevc-finetune\n",
    "# ! CUDA_VISIBLE_DEVICES=0 python convert.py --hpfile configs/freevc.json --ptfile checkpoints/freevc.pth --txtpath convert.txt --outdir outputs/freevc-base\n",
    "! CUDA_VISIBLE_DEVICES=0 python convert.py --hpfile configs/freevc-finetune.json --ptfile logs/freevc-finetune/G_40.pth --txtpath convert.txt --outdir outputs/freevc-finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import IPython\n",
    "ref_1 = \"\"\n",
    "print(\"Ref 1\")\n",
    "IPython.display.display(ipd.Audio(ref_1.numpy(), rate=sr))\n",
    "print(\"Example 1\")\n",
    "IPython.display.display(ipd.Audio(example_1.numpy(), rate=sr))\n",
    "print(\"Example 2\")\n",
    "IPython.display.display(ipd.Audio(example_2.numpy(), rate=sr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE LATER, JUST STATS\n",
    "\n",
    "import os\n",
    "def test_split(total_len):\n",
    "    n_test = max(round(total_len*0.05), 1)\n",
    "    n_val = max(round(total_len*0.01), 1)\n",
    "    n_train = total_len-(n_test+n_val)\n",
    "\n",
    "    assert total_len == n_test+n_val+n_train\n",
    "    print(total_len, \":\\t\",n_train,\", \",n_test,\", \", n_val)\n",
    "    assert total_len>=10, \"message something\"\n",
    "\n",
    "dir = os.path.abspath(\"~/\")\n",
    "os.walk(dir)\n",
    "vctk_path = os.path.abspath(\"../../../../../../mnt/c/Users/mhess/Downloads/VCTK-Corpus-0.92/wav48_silence_trimmed\")\n",
    "dirlist = os.listdir(vctk_path)\n",
    "# TODO: get avg number of chunks/speaker in vctk\n",
    "counter = 0\n",
    "num_chunks = 0\n",
    "for el in dirlist:\n",
    "    combined_path = os.path.join(vctk_path,el)\n",
    "    # print(f'{el}:\\t{os.path.isdir(combined_path)}')\n",
    "    if os.path.isdir(combined_path):\n",
    "        counter += 1\n",
    "        num_chunks += (len(os.listdir(combined_path)))/2\n",
    "\n",
    "print(f'AVG chunks per speaker: {round(num_chunks/counter, 2)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-freevc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
